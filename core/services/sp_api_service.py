# sp_api_service.py - Prime+å‡ºå“è€…æƒ…å ±çµ±åˆãƒ•ãƒ«ã‚³ãƒ¼ãƒ‰ç‰ˆï¼ˆPhase 4.0å¯¾å¿œç‰ˆï¼‰
import time
import os
from dotenv import load_dotenv
import re
import pandas as pd
import json
import unicodedata
from pathlib import Path
import numpy as np
import traceback
import logging

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ğŸ†• Phase 4.0: è¨­å®šç®¡ç†ã‚·ã‚¹ãƒ†ãƒ çµ±åˆ
CONFIG_MANAGER_AVAILABLE = False
try:
    from config_manager import create_threshold_config_manager
    config_manager = create_threshold_config_manager()
    CONFIG_MANAGER_AVAILABLE = True
    logger.info("âœ… config_manager.py çµ±åˆæˆåŠŸ - å‹•çš„ã‚¹ã‚³ã‚¢èª¿æ•´ãŒåˆ©ç”¨å¯èƒ½")
    
    # ç¾åœ¨ã®è¨­å®šæƒ…å ±ã‚’è¡¨ç¤º
    current_preset = config_manager.current_config.get("applied_preset", "ã‚«ã‚¹ã‚¿ãƒ ")
    last_updated = config_manager.current_config.get("last_updated", "ä¸æ˜")
    logger.info(f"âš™ï¸ ç¾åœ¨ã®è¨­å®š: ãƒ—ãƒªã‚»ãƒƒãƒˆ='{current_preset}', æœ€çµ‚æ›´æ–°={last_updated}")
    
except ImportError as e:
    logger.warning(f"âš ï¸ config_manager.py ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¤±æ•—: {e}")
    logger.info("ğŸ“‹ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã•ã‚ŒãŸã‚¹ã‚³ã‚¢è¨­å®šã‚’ä½¿ç”¨")

def get_config_value(category: str, key: str, fallback_value):
    """
    è¨­å®šå€¤ã®å–å¾—ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¯¾å¿œï¼‰
    
    Args:
        category: è¨­å®šã‚«ãƒ†ã‚´ãƒª
        key: è¨­å®šã‚­ãƒ¼
        fallback_value: ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å€¤
        
    Returns:
        è¨­å®šå€¤ã¾ãŸã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å€¤
    """
    if CONFIG_MANAGER_AVAILABLE:
        try:
            value = config_manager.get_threshold(category, key, fallback_value)
            logger.debug(f"è¨­å®šå€¤å–å¾—: {category}.{key} = {value}")
            return value
        except Exception as e:
            logger.error(f"âš ï¸ è¨­å®šå€¤å–å¾—ã‚¨ãƒ©ãƒ¼ {category}.{key}: {e}")
            return fallback_value
    else:
        return fallback_value

# .envèª­ã¿è¾¼ã¿
current_dir = Path(__file__).parent
parent_dir = current_dir.parent
env_path = parent_dir / '.env'

if not env_path.exists():
    env_path_alt = Path.cwd() / '.env'
    if env_path_alt.exists():
        env_path = env_path_alt
    else:
        logger.warning(f"Warning: .env file not found at {env_path} or {env_path_alt}")
load_dotenv(env_path)

def get_japanese_name_from_gpt4o(clean_title):
    """GPT-4oã«ã‚ˆã‚‹é«˜å“è³ªæ—¥æœ¬èªåŒ–"""
    try:
        import openai
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            return None, "OpenAI API Key not found"
        client = openai.OpenAI(api_key=api_key)
        prompt = f"æ¬¡ã®è‹±èªã®å•†å“åã‚’ã€æ—¥æœ¬ã®ECã‚µã‚¤ãƒˆã§é€šã˜ã‚‹è‡ªç„¶ãªæ—¥æœ¬èªã®å•†å“åã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚å„å˜èªã¯åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ã§åŒºåˆ‡ã‚Šã€ãƒ–ãƒ©ãƒ³ãƒ‰ã‚„å®¹é‡ã‚‚æ—¥æœ¬èªã§è¡¨è¨˜ã—ã€èª¬æ˜ã‚„ä½™è¨ˆãªèªå¥ã¯ä¸è¦ï¼š\n\n{clean_title}"
        response = client.chat.completions.create(
            model="gpt-4o", messages=[{"role": "user", "content": prompt}], max_tokens=64, temperature=0.3,
        )
        japanese_name = response.choices[0].message.content.strip()
        return japanese_name, "GPT-4o"
    except Exception as e:
        return None, f"GPT-4o Error: {e}"

def get_japanese_name_from_gemini(clean_title):
    """Geminiã«ã‚ˆã‚‹æ—¥æœ¬èªåŒ–ï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç”¨ï¼‰"""
    try:
        import google.generativeai as genai
        api_key = os.getenv('GEMINI_API_KEY')
        if not api_key:
            return None, "Gemini API Key not found"
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-1.5-pro')
        prompt = f"æ¬¡ã®è‹±èªã®å•†å“åã‚’ã€æ—¥æœ¬èªã®å•†å“åã«è‡ªç„¶ã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚ãƒ–ãƒ©ãƒ³ãƒ‰ã‚„å®¹é‡ã‚‚è‡ªç„¶ãªæ—¥æœ¬èªã§ã€‚ä½™è¨ˆãªèª¬æ˜ä¸è¦ï¼š\n\n{clean_title}"
        response = model.generate_content(prompt)
        japanese_name = response.text.strip()
        return japanese_name, "Gemini"
    except Exception as e:
        return None, f"Gemini Error: {e}"

def get_japanese_name_hybrid(clean_title):
    """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ—¥æœ¬èªåŒ–ï¼ˆGPT-4o + Geminiãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼‰"""
    jp_name, source = get_japanese_name_from_gpt4o(clean_title)
    if jp_name and not jp_name.isspace() and "å¤‰æ›ä¸å¯" not in jp_name:
        return jp_name, source
    jp_name, source = get_japanese_name_from_gemini(clean_title)
    if jp_name and not jp_name.isspace() and "å¤‰æ›ä¸å¯" not in jp_name:
        return jp_name, source
    return clean_title, "Original"

def load_brand_dict():
    """é«˜å“è³ªãƒ–ãƒ©ãƒ³ãƒ‰è¾æ›¸ã®èª­ã¿è¾¼ã¿"""
    try:
        base_path = Path.cwd() if not Path(__file__).parent.exists() else Path(__file__).parent
        brands_file_path = base_path.parent / 'data' / 'brands.json'
        if not brands_file_path.exists():
             brands_file_path = base_path / 'data' / 'brands.json'
        if brands_file_path.exists():
            with open(brands_file_path, 'r', encoding='utf-8') as f: 
                existing_brands = json.load(f)
            print(f"ğŸ“š æ—¢å­˜brands.jsonã‹ã‚‰èª­ã¿è¾¼ã¿: {len(existing_brands)}ãƒ–ãƒ©ãƒ³ãƒ‰ (Path: {brands_file_path})")
            return existing_brands
        else: 
            print(f"âš ï¸ brands.jsonãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {brands_file_path}")
    except Exception as e:
        print(f"âš ï¸ brands.jsonèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}ã€‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¾æ›¸ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    
    fallback_brands = {"FANCL": ["ãƒ•ã‚¡ãƒ³ã‚±ãƒ«", "fancl"], "ORBIS": ["ã‚ªãƒ«ãƒ“ã‚¹", "orbis"], "SK-II": ["ã‚¨ã‚¹ã‚±ãƒ¼ãƒ„ãƒ¼", "SK2", "SK-2"], "SHISEIDO": ["è³‡ç”Ÿå ‚", "shiseido"], "KANEBO": ["ã‚«ãƒãƒœã‚¦", "kanebo"], "KOSE": ["ã‚³ãƒ¼ã‚»ãƒ¼", "kose"], "POLA": ["ãƒãƒ¼ãƒ©", "pola"], "ALBION": ["ã‚¢ãƒ«ãƒ“ã‚ªãƒ³", "albion"], "HABA": ["ãƒãƒ¼ãƒãƒ¼", "haba"], "DHC": ["ãƒ‡ã‚£ãƒ¼ã‚¨ã‚¤ãƒã‚·ãƒ¼", "dhc"], "MILBON": ["ãƒŸãƒ«ãƒœãƒ³", "milbon"], "LEBEL": ["ãƒ«ãƒ™ãƒ«", "lebel"], "YOLU": ["ãƒ¨ãƒ«", "yolu"], "TSUBAKI": ["æ¤¿", "tsubaki"], "LISSAGE": ["ãƒªã‚µãƒ¼ã‚¸", "lissage"], "KERASTASE": ["ã‚±ãƒ©ã‚¹ã‚¿ãƒ¼ã‚¼", "kerastase"], "PANASONIC": ["ãƒ‘ãƒŠã‚½ãƒ‹ãƒƒã‚¯", "panasonic"], "PHILIPS": ["ãƒ•ã‚£ãƒªãƒƒãƒ—ã‚¹", "philips"], "KOIZUMI": ["ã‚³ã‚¤ã‚ºãƒŸ", "koizumi"], "HITACHI": ["æ—¥ç«‹", "hitachi"], "SUNTORY": ["ã‚µãƒ³ãƒˆãƒªãƒ¼", "suntory"], "ASAHI": ["ã‚¢ã‚µãƒ’", "asahi"], "MEIJI": ["æ˜æ²»", "meiji"], "MORINAGA": ["æ£®æ°¸", "morinaga"],}
    print(f"ğŸ“š ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¾æ›¸ä½¿ç”¨: {len(fallback_brands)}ãƒ–ãƒ©ãƒ³ãƒ‰")
    return fallback_brands

def load_beauty_terms_dict():
    """ç¾å®¹ãƒ»åŒ–ç²§å“å°‚é–€ç”¨èªã®è‹±æ—¥ãƒãƒƒãƒ”ãƒ³ã‚°è¾æ›¸"""
    return {
        # åŸºæœ¬ã‚¹ã‚­ãƒ³ã‚±ã‚¢ç”¨èª
        "oil": ["ã‚ªã‚¤ãƒ«", "æ²¹"],
        "cream": ["ã‚¯ãƒªãƒ¼ãƒ "],
        "lotion": ["ãƒ­ãƒ¼ã‚·ãƒ§ãƒ³", "åŒ–ç²§æ°´"],
        "serum": ["ã‚»ãƒ©ãƒ ", "ç¾å®¹æ¶²"],
        "essence": ["ã‚¨ãƒƒã‚»ãƒ³ã‚¹", "ç¾å®¹æ¶²"],
        "toner": ["ãƒˆãƒŠãƒ¼", "åŒ–ç²§æ°´", "åã‚Œã‚“åŒ–ç²§æ°´"],
        "cleanser": ["ã‚¯ãƒ¬ãƒ³ã‚¶ãƒ¼", "æ´—é¡”æ–™"],
        "cleansing": ["ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°", "æ´—æµ„"],
        "facial": ["ãƒ•ã‚§ã‚¤ã‚·ãƒ£ãƒ«", "é¡”ç”¨", "æ´—é¡”"],
        "moisturizer": ["ãƒ¢ã‚¤ã‚¹ãƒãƒ£ãƒ©ã‚¤ã‚¶ãƒ¼", "ä¿æ¹¿å‰¤", "ä¿æ¹¿ã‚¯ãƒªãƒ¼ãƒ "],
        "emulsion": ["ã‚¨ãƒãƒ«ã‚¸ãƒ§ãƒ³", "ä¹³æ¶²"],
        "gel": ["ã‚¸ã‚§ãƒ«"],
        "foam": ["ãƒ•ã‚©ãƒ¼ãƒ ", "æ³¡"],
        "scrub": ["ã‚¹ã‚¯ãƒ©ãƒ–"],
        "mask": ["ãƒã‚¹ã‚¯", "ãƒ‘ãƒƒã‚¯"],
        "pack": ["ãƒ‘ãƒƒã‚¯", "ãƒã‚¹ã‚¯"],
        "sheet": ["ã‚·ãƒ¼ãƒˆ"],
        "patch": ["ãƒ‘ãƒƒãƒ"],
        
        # ãƒ˜ã‚¢ã‚±ã‚¢ç”¨èª
        "shampoo": ["ã‚·ãƒ£ãƒ³ãƒ—ãƒ¼"],
        "conditioner": ["ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒŠãƒ¼", "ãƒªãƒ³ã‚¹"],
        "treatment": ["ãƒˆãƒªãƒ¼ãƒˆãƒ¡ãƒ³ãƒˆ", "ãƒ˜ã‚¢ãƒˆãƒªãƒ¼ãƒˆãƒ¡ãƒ³ãƒˆ"],
        "hair": ["ãƒ˜ã‚¢", "é«ª", "æ¯›é«ª"],
        "scalp": ["ã‚¹ã‚«ãƒ«ãƒ—", "é ­çš®"],
        "repair": ["ãƒªãƒšã‚¢", "ä¿®å¾©"],
        "moisture": ["ãƒ¢ã‚¤ã‚¹ãƒãƒ£ãƒ¼", "ä¿æ¹¿"],
        "volume": ["ãƒœãƒªãƒ¥ãƒ¼ãƒ "],
        "smooth": ["ã‚¹ãƒ ãƒ¼ã‚¹", "æ»‘ã‚‰ã‹"],
        "silky": ["ã‚·ãƒ«ã‚­ãƒ¼", "çµ¹ã®ã‚ˆã†ãª"],
        "straight": ["ã‚¹ãƒˆãƒ¬ãƒ¼ãƒˆ", "ç›´æ¯›"],
        "curl": ["ã‚«ãƒ¼ãƒ«", "å·»ãé«ª"],
        "wave": ["ã‚¦ã‚§ãƒ¼ãƒ–"],
        
        # ãƒ¡ã‚¤ã‚¯ã‚¢ãƒƒãƒ—ç”¨èª
        "foundation": ["ãƒ•ã‚¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³", "ä¸‹åœ°"],
        "concealer": ["ã‚³ãƒ³ã‚·ãƒ¼ãƒ©ãƒ¼"],
        "powder": ["ãƒ‘ã‚¦ãƒ€ãƒ¼", "ç²‰"],
        "blush": ["ãƒãƒ¼ã‚¯", "é ¬ç´…"],
        "eyeshadow": ["ã‚¢ã‚¤ã‚·ãƒ£ãƒ‰ã‚¦"],
        "eyeliner": ["ã‚¢ã‚¤ãƒ©ã‚¤ãƒŠãƒ¼"],
        "mascara": ["ãƒã‚¹ã‚«ãƒ©"],
        "lipstick": ["ãƒªãƒƒãƒ—ã‚¹ãƒ†ã‚£ãƒƒã‚¯", "å£ç´…"],
        "lip": ["ãƒªãƒƒãƒ—", "å”‡"],
        "gloss": ["ã‚°ãƒ­ã‚¹", "ã¤ã‚„"],
        "rouge": ["ãƒ«ãƒ¼ã‚¸ãƒ¥", "å£ç´…"],
        "primer": ["ãƒ—ãƒ©ã‚¤ãƒãƒ¼", "ä¸‹åœ°"],
        "highlighter": ["ãƒã‚¤ãƒ©ã‚¤ã‚¿ãƒ¼"],
        "bronzer": ["ãƒ–ãƒ­ãƒ³ã‚¶ãƒ¼"],
        
        # åŠ¹æœãƒ»ç‰¹æ€§ç”¨èª
        "hydrating": ["ãƒã‚¤ãƒ‰ãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°", "ä¿æ¹¿", "æ½¤ã„"],
        "moisturizing": ["ãƒ¢ã‚¤ã‚¹ãƒãƒ£ãƒ©ã‚¤ã‚¸ãƒ³ã‚°", "ä¿æ¹¿"],
        "nourishing": ["ãƒŠãƒªã‚·ãƒ³ã‚°", "æ „é¤Š", "æ»‹é¤Š"],
        "anti-aging": ["ã‚¢ãƒ³ãƒã‚¨ã‚¤ã‚¸ãƒ³ã‚°", "ã‚¨ã‚¤ã‚¸ãƒ³ã‚°ã‚±ã‚¢", "è€åŒ–é˜²æ­¢"],
        "brightening": ["ãƒ–ãƒ©ã‚¤ãƒˆãƒ‹ãƒ³ã‚°", "ç¾ç™½", "æ˜ã‚‹ã"],
        "whitening": ["ãƒ›ãƒ¯ã‚¤ãƒˆãƒ‹ãƒ³ã‚°", "ç¾ç™½"],
        "firming": ["ãƒ•ã‚¡ãƒ¼ãƒŸãƒ³ã‚°", "å¼•ãç· ã‚"],
        "lifting": ["ãƒªãƒ•ãƒ†ã‚£ãƒ³ã‚°", "ãƒªãƒ•ãƒˆã‚¢ãƒƒãƒ—"],
        "regenerating": ["ãƒªã‚¸ã‚§ãƒãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°", "å†ç”Ÿ"],
        "revitalizing": ["ãƒªãƒã‚¤ã‚¿ãƒ©ã‚¤ã‚¸ãƒ³ã‚°", "æ´»æ€§åŒ–"],
        "soothing": ["ã‚¹ãƒ¼ã‚¸ãƒ³ã‚°", "é®é™", "è½ã¡ç€ã‹ã›ã‚‹"],
        "calming": ["ã‚«ãƒ¼ãƒŸãƒ³ã‚°", "é®é™"],
        "sensitive": ["ã‚»ãƒ³ã‚·ãƒ†ã‚£ãƒ–", "æ•æ„Ÿ"],
        "gentle": ["ã‚¸ã‚§ãƒ³ãƒˆãƒ«", "ã‚„ã•ã—ã„", "ç©ã‚„ã‹"],
        "mild": ["ãƒã‚¤ãƒ«ãƒ‰", "ç©ã‚„ã‹", "ã‚„ã•ã—ã„"],
        "deep": ["ãƒ‡ã‚£ãƒ¼ãƒ—", "æ·±ã„", "æ¿ƒåš"],
        "intensive": ["ã‚¤ãƒ³ãƒ†ãƒ³ã‚·ãƒ–", "é›†ä¸­"],
        "ultra": ["ã‚¦ãƒ«ãƒˆãƒ©", "è¶…"],
        "super": ["ã‚¹ãƒ¼ãƒ‘ãƒ¼", "è¶…"],
        "premium": ["ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ", "é«˜ç´š"],
        "professional": ["ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«", "ãƒ—ãƒ­ç”¨"],
        "organic": ["ã‚ªãƒ¼ã‚¬ãƒ‹ãƒƒã‚¯", "æœ‰æ©Ÿ"],
        "natural": ["ãƒŠãƒãƒ¥ãƒ©ãƒ«", "å¤©ç„¶"],
        
        # è‚Œè³ªãƒ»é«ªè³ªç”¨èª
        "oily": ["ã‚ªã‚¤ãƒªãƒ¼", "è„‚æ€§"],
        "dry": ["ãƒ‰ãƒ©ã‚¤", "ä¹¾ç‡¥"],
        "combination": ["ã‚³ãƒ³ãƒ“ãƒãƒ¼ã‚·ãƒ§ãƒ³", "æ··åˆè‚Œ"],
        "normal": ["ãƒãƒ¼ãƒãƒ«", "æ™®é€šè‚Œ"],
        "mature": ["ãƒãƒãƒ¥ã‚¢", "æˆç†Ÿè‚Œ"],
        "damaged": ["ãƒ€ãƒ¡ãƒ¼ã‚¸", "å‚·ã‚“ã "],
        "colored": ["ã‚«ãƒ©ãƒ¼", "æŸ“ã‚ãŸ"],
        "bleached": ["ãƒ–ãƒªãƒ¼ãƒ", "è„±è‰²ã—ãŸ"],
        "fine": ["ãƒ•ã‚¡ã‚¤ãƒ³", "ç´°ã„"],
        "thick": ["ã‚·ãƒƒã‚¯", "å¤ªã„"],
        "thin": ["ã‚·ãƒ³", "è–„ã„"],
        
        # å®¹å™¨ãƒ»å½¢çŠ¶ç”¨èª
        "pump": ["ãƒãƒ³ãƒ—"],
        "tube": ["ãƒãƒ¥ãƒ¼ãƒ–"],
        "bottle": ["ãƒœãƒˆãƒ«"],
        "jar": ["ã‚¸ãƒ£ãƒ¼", "å£º"],
        "compact": ["ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆ"],
        "palette": ["ãƒ‘ãƒ¬ãƒƒãƒˆ"],
        "stick": ["ã‚¹ãƒ†ã‚£ãƒƒã‚¯"],
        "pen": ["ãƒšãƒ³"],
        "pencil": ["ãƒšãƒ³ã‚·ãƒ«"],
        "brush": ["ãƒ–ãƒ©ã‚·"],
        "sponge": ["ã‚¹ãƒãƒ³ã‚¸"],
        "applicator": ["ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚¿ãƒ¼"],
        
        # ãƒ–ãƒ©ãƒ³ãƒ‰ãƒ»ã‚·ãƒªãƒ¼ã‚ºç”¨èª
        "collection": ["ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³"],
        "series": ["ã‚·ãƒªãƒ¼ã‚º"],
        "line": ["ãƒ©ã‚¤ãƒ³"],
        "range": ["ãƒ¬ãƒ³ã‚¸"],
        "edition": ["ã‚¨ãƒ‡ã‚£ã‚·ãƒ§ãƒ³"],
        "limited": ["ãƒªãƒŸãƒ†ãƒƒãƒ‰", "é™å®š"],
        "special": ["ã‚¹ãƒšã‚·ãƒ£ãƒ«", "ç‰¹åˆ¥"],
        "deluxe": ["ãƒ‡ãƒ©ãƒƒã‚¯ã‚¹", "è±ªè¯"],
        "mini": ["ãƒŸãƒ‹", "å°ã•ã„"],
        "travel": ["ãƒˆãƒ©ãƒ™ãƒ«", "æ—…è¡Œç”¨"],
        "set": ["ã‚»ãƒƒãƒˆ"],
        "kit": ["ã‚­ãƒƒãƒˆ"],
        "duo": ["ãƒ‡ãƒ¥ã‚ª", "2å€‹ã‚»ãƒƒãƒˆ"],
        "trio": ["ãƒˆãƒªã‚ª", "3å€‹ã‚»ãƒƒãƒˆ"]
    }

def advanced_product_name_cleansing(text):
    """é«˜å“è³ªå•†å“åã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°"""
    if not text: return ""
    text = unicodedata.normalize('NFKC', text)
    remove_patterns = [r'[ğŸ…¹ğŸ…¿ğŸ‡¯ğŸ‡µâ˜…â˜†â€»â—â—‹â—â–²â–³â–¼â–½â– â–¡â—†â—‡â™¦â™¢â™ â™£â™¥â™¡]', r'[\u2600-\u26FF\u2700-\u27BF]', r'\[.*?stock.*?\]', r'\[.*?åœ¨åº«.*?\]', r'é€æ–™ç„¡æ–™', r'é…é€ç„¡æ–™', r'Free shipping', r'100% Authentic', r'made in japan', r'original', r'Direct from japan', r'Guaranteed authentic', r'æ­£è¦å“', r'æœ¬ç‰©', r'æ–°å“', r'æœªä½¿ç”¨', r'@.*', r'by.*store', r'shop.*', r'hair care liquid', r'beauty product', r'cosmetic',]
    for pattern in remove_patterns: text = re.sub(pattern, '', text, flags=re.IGNORECASE)
    text = re.sub(r'^(Japan|Global|Korean|China)\s+', '', text, flags=re.IGNORECASE)
    if '/' in text and len(text.split('/')) > 1: text = text.split('/')[0].strip()
    text = re.sub(r'\s+', ' ', text).strip(); text = re.sub(r'^[^\w\s]*|[^\w\s]*$', '', text)
    return text

def clean_product_name(title):
    """å•†å“åã®ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å‡¦ç†"""
    if not title:
        return ""
    
    # åŸºæœ¬çš„ãªã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
    title = title.strip()
    title = re.sub(r'\s+', ' ', title)  # è¤‡æ•°ã®ç©ºç™½ã‚’1ã¤ã«
    
    # ç‰¹æ®Šæ–‡å­—ã®æ­£è¦åŒ–
    title = unicodedata.normalize('NFKC', title)
    
    # ä¸è¦ãªæ–‡å­—ã®å‰Šé™¤
    title = re.sub(r'[^\w\s\-.,&()/]', '', title)
    
    # å¤§æ–‡å­—å°æ–‡å­—ã®æ­£è¦åŒ–
    title = title.lower()
    
    return title

def extract_brand_and_quantity(title, brand_dict):
    """å•†å“åã‹ã‚‰ãƒ–ãƒ©ãƒ³ãƒ‰åã¨æ•°é‡ã‚’æŠ½å‡º"""
    if not title:
        return None, None
    
    # å•†å“åã®ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
    cleaned_title = clean_product_name(title)
    
    # ãƒ–ãƒ©ãƒ³ãƒ‰åã®æŠ½å‡º
    brand_name = None
    for brand, variations in brand_dict.items():
        for variation in variations:
            if variation.lower() in cleaned_title:
                brand_name = brand
                break
        if brand_name:
            break
    
    # æ•°é‡ã®æŠ½å‡º
    quantity = None
    quantity_patterns = [
        r'(\d+)\s*(?:ml|g|oz|fl\.?\s*oz|piece|pcs|set|pack|bottle|tube|jar|box)',
        r'(?:ml|g|oz|fl\.?\s*oz|piece|pcs|set|pack|bottle|tube|jar|box)\s*(\d+)',
        r'(\d+)\s*(?:x|Ã—)\s*\d+',
        r'(\d+)\s*(?:count|ct)'
    ]
    
    for pattern in quantity_patterns:
        match = re.search(pattern, cleaned_title, re.IGNORECASE)
        if match:
            quantity = match.group(1)
            break
    
    return brand_name, quantity

def _calculate_prime_confidence_score_internal(prime_info_dict):
    """Primeæƒ…å ±ã®ä¿¡é ¼æ€§ã‚¹ã‚³ã‚¢è¨ˆç®— (å†…éƒ¨ãƒ˜ãƒ«ãƒ‘ãƒ¼ã€è¾æ›¸ã‚’å—ã‘å–ã‚‹)"""
    confidence_score = 50; seller_name = str(prime_info_dict.get('seller_name', ''))
    if 'Amazon.co.jp' in seller_name or 'Amazon' == seller_name: confidence_score += 30
    elif 'æ¨å®š' in seller_name or 'Estimated' in seller_name: confidence_score -= 30
    elif seller_name and seller_name.lower() != 'nan': confidence_score += 10
    asin_val = str(prime_info_dict.get('asin', ''));
    if asin_val.startswith('B0DR') or asin_val.startswith('B0DS'): confidence_score -= 20
    elif asin_val.startswith('B00') or asin_val.startswith('B01'): confidence_score += 10
    seller_type = str(prime_info_dict.get('seller_type', '')).lower(); is_prime = prime_info_dict.get('is_prime', False)
    if is_prime and seller_type == 'amazon': confidence_score += 20
    elif is_prime and seller_type == 'official_manufacturer': confidence_score += 15
    elif is_prime and seller_type == 'third_party': confidence_score += 5
    elif not is_prime and seller_type == 'amazon': confidence_score -= 25
    ship_hours = prime_info_dict.get('ship_hours')
    if ship_hours is not None:
        if ship_hours <= 12: confidence_score += 15
        elif ship_hours <= 24: confidence_score += 10
        elif ship_hours <= 48: confidence_score += 5
    return max(0, min(100, confidence_score))

def get_prime_and_seller_info_v8_enhanced(asin, brand_name=''):
    """Primeæƒ…å ±ã¨å‡ºå“è€…æƒ…å ±ã‚’å–å¾—ï¼ˆPhase 4.0ï¼‰"""
    logger.info(f"ğŸ” Primeæƒ…å ±å–å¾—é–‹å§‹: ASIN={asin}, brand={brand_name}")
    
    # è¨­å®šå€¤ã®å–å¾—
    prime_confidence_threshold = get_config_value('prime', 'confidence_threshold', 0.7)
    ship_hours_threshold = get_config_value('prime', 'ship_hours_threshold', 48)
    seller_score_threshold = get_config_value('prime', 'seller_score_threshold', 0.8)
    
    logger.info(f"âš™ï¸ è¨­å®šå€¤: prime_confidence={prime_confidence_threshold}, "
                f"ship_hours={ship_hours_threshold}, seller_score={seller_score_threshold}")
    
    # ASINã®æ¤œè¨¼
    if not asin or not re.match(r'^[A-Z0-9]{10}$', asin):
        logger.warning(f"âš ï¸ ç„¡åŠ¹ãªASIN: {asin}")
        return {
            'is_prime': False,
            'prime_confidence': 0.0,
            'prime_reason': 'Invalid ASIN',
            'seller_info': {},
            'shipping_info': {},
            'shopee_score_bonus': 0.0
        }
    
    try:
        # å‡ºå“è€…æƒ…å ±ã®å–å¾—
        seller_info = get_seller_info(asin)
        logger.info(f"ğŸ‘¤ å‡ºå“è€…æƒ…å ±: {seller_info}")
        
        # é…é€æƒ…å ±ã®å–å¾—
        shipping_info = get_shipping_info(asin)
        logger.info(f"ğŸšš é…é€æƒ…å ±: {shipping_info}")
        
        # Primeåˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯
        prime_info = {
            'is_prime': False,
            'prime_confidence': 0.0,
            'prime_reason': '',
            'seller_info': seller_info,
            'shipping_info': shipping_info,
            'shopee_score_bonus': 0.0
        }
        
        # å‡ºå“è€…ã‚¹ã‚³ã‚¢ã®è¨ˆç®—
        seller_score = 0.0
        if seller_info.get('is_amazon'):
            seller_score = 1.0
        elif seller_info.get('is_official'):
            seller_score = 0.9
        elif seller_info.get('is_authorized'):
            seller_score = 0.8
        elif seller_info.get('is_verified'):
            seller_score = 0.7
        elif seller_info.get('is_third_party'):
            seller_score = 0.5
        
        # é…é€ã‚¹ã‚³ã‚¢ã®è¨ˆç®—
        shipping_score = 0.0
        if shipping_info.get('is_prime'):
            shipping_score = 1.0
        elif shipping_info.get('is_fba'):
            shipping_score = 0.9
        elif shipping_info.get('ship_hours', float('inf')) <= ship_hours_threshold:
            shipping_score = 0.8
        else:
            shipping_score = 0.5
        
        # ç·åˆã‚¹ã‚³ã‚¢ã®è¨ˆç®—
        total_score = (seller_score * 0.6) + (shipping_score * 0.4)
        prime_info['prime_confidence'] = total_score
        
        # Primeåˆ¤å®š
        if total_score >= prime_confidence_threshold:
            prime_info['is_prime'] = True
            prime_info['prime_reason'] = 'High confidence score'
            prime_info['shopee_score_bonus'] = 0.1
        else:
            prime_info['prime_reason'] = 'Low confidence score'
        
        logger.info(f"âœ… Primeåˆ¤å®šå®Œäº†: is_prime={prime_info['is_prime']}, "
                   f"confidence={prime_info['prime_confidence']:.2f}, "
                   f"reason={prime_info['prime_reason']}")
        
        return prime_info
        
    except Exception as e:
        logger.error(f"âŒ Primeæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return {
            'is_prime': False,
            'prime_confidence': 0.0,
            'prime_reason': f'Error: {str(e)}',
            'seller_info': {},
            'shipping_info': {},
            'shopee_score_bonus': 0.0
        }

def calculate_prime_score(prime_info):
    """ Primeã‚¹ã‚³ã‚¢ã®è¨ˆç®—ï¼ˆPhase 4.0å¯¾å¿œç‰ˆï¼‰ """
    try:
        # è¨­å®šå€¤ã®å–å¾—
        seller_weight = get_config_value("prime_settings", "seller_weight", 0.4)
        shipping_weight = get_config_value("prime_settings", "shipping_weight", 0.4)
        brand_weight = get_config_value("prime_settings", "brand_weight", 0.2)
        
        # å‡ºå“è€…ã‚¹ã‚³ã‚¢ã®è¨ˆç®—
        seller_score = 0
        if prime_info.get('is_amazon_seller'):
            seller_score += 40
        if prime_info.get('is_official_seller'):
            seller_score += 30
        if prime_info.get('is_fba'):
            seller_score += 30
        
        # é…é€ã‚¹ã‚³ã‚¢ã®è¨ˆç®—
        shipping_score = 0
        ship_hours = prime_info.get('ship_hours')
        if ship_hours is not None:
            if ship_hours <= 24:
                shipping_score = 100
            elif ship_hours <= 48:
                shipping_score = 80
            elif ship_hours <= 72:
                shipping_score = 60
            else:
                shipping_score = 40
        
        # ãƒ–ãƒ©ãƒ³ãƒ‰ã‚¹ã‚³ã‚¢ã®è¨ˆç®—
        brand_score = 0
        if prime_info.get('seller_type') == 'official':
            brand_score = 100
        elif prime_info.get('seller_type') == 'authorized':
            brand_score = 80
        elif prime_info.get('seller_type') == 'verified':
            brand_score = 60
        
        # ç·åˆã‚¹ã‚³ã‚¢ã®è¨ˆç®—
        total_score = (
            seller_score * seller_weight +
            shipping_score * shipping_weight +
            brand_score * brand_weight
        )
        
        logger.debug(f"ğŸ“Š ã‚¹ã‚³ã‚¢è¨ˆç®—: å‡ºå“è€…={seller_score}, é…é€={shipping_score}, ãƒ–ãƒ©ãƒ³ãƒ‰={brand_score}, ç·åˆ={total_score}")
        return round(total_score)
        
    except Exception as e:
        logger.error(f"âŒ Primeã‚¹ã‚³ã‚¢è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
        return 25  # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å€¤

def calculate_shopee_suitability_score(product_info, prime_info):
    """Shopeeé©æ€§ã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""
    try:
        # åŸºæœ¬ã‚¹ã‚³ã‚¢ã®åˆæœŸåŒ–
        base_score = 0.0
        
        # ãƒ–ãƒ©ãƒ³ãƒ‰ã‚¹ã‚³ã‚¢ã®è¨ˆç®—
        brand_score = 0.0
        if product_info.get('extracted_brand'):
            brand_score = 0.3  # ãƒ–ãƒ©ãƒ³ãƒ‰ãŒç‰¹å®šã§ããŸå ´åˆã®ãƒœãƒ¼ãƒŠã‚¹
        
        # Primeã‚¹ã‚³ã‚¢ã®è¨ˆç®—
        prime_score = 0.0
        if prime_info.get('is_prime'):
            prime_score = 0.4  # Primeå•†å“ã®ãƒœãƒ¼ãƒŠã‚¹
        prime_score += prime_info.get('shopee_score_bonus', 0.0)  # è¿½åŠ ãƒœãƒ¼ãƒŠã‚¹
        
        # é…é€ã‚¹ã‚³ã‚¢ã®è¨ˆç®—
        shipping_score = 0.0
        if prime_info.get('shipping_info', {}).get('is_prime'):
            shipping_score = 0.2
        elif prime_info.get('shipping_info', {}).get('is_fba'):
            shipping_score = 0.15
        elif prime_info.get('shipping_info', {}).get('ship_hours', float('inf')) <= 48:
            shipping_score = 0.1
        
        # å‡ºå“è€…ã‚¹ã‚³ã‚¢ã®è¨ˆç®—
        seller_score = 0.0
        if prime_info.get('seller_info', {}).get('is_amazon'):
            seller_score = 0.1
        elif prime_info.get('seller_info', {}).get('is_official'):
            seller_score = 0.08
        elif prime_info.get('seller_info', {}).get('is_authorized'):
            seller_score = 0.05
        
        # ç·åˆã‚¹ã‚³ã‚¢ã®è¨ˆç®—
        total_score = base_score + brand_score + prime_score + shipping_score + seller_score
        
        # ã‚¹ã‚³ã‚¢ã®æ­£è¦åŒ–ï¼ˆ0.0-1.0ã®ç¯„å›²ã«ï¼‰
        normalized_score = min(max(total_score, 0.0), 1.0)
        
        return normalized_score
        
    except Exception as e:
        logger.error(f"âŒ Shopeeé©æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return 0.0

def calculate_shopee_bonus(prime_info, brand_name):
    """ Shopeeã‚¹ã‚³ã‚¢ãƒœãƒ¼ãƒŠã‚¹ã®è¨ˆç®—ï¼ˆPhase 4.0å¯¾å¿œç‰ˆï¼‰ """
    try:
        # è¨­å®šå€¤ã®å–å¾—
        prime_bonus = get_config_value("shopee_settings", "prime_bonus", 20)
        shipping_bonus = get_config_value("shopee_settings", "shipping_bonus", 15)
        brand_bonus = get_config_value("shopee_settings", "brand_bonus", 10)
        
        bonus = 0
        
        # Primeãƒœãƒ¼ãƒŠã‚¹
        if prime_info.get('is_prime'):
            bonus += prime_bonus
        
        # é…é€ãƒœãƒ¼ãƒŠã‚¹
        ship_hours = prime_info.get('ship_hours')
        if ship_hours is not None and ship_hours <= 24:
            bonus += shipping_bonus
        
        # ãƒ–ãƒ©ãƒ³ãƒ‰ãƒœãƒ¼ãƒŠã‚¹
        if prime_info.get('seller_type') in ['official', 'authorized']:
            bonus += brand_bonus
        
        logger.debug(f"ğŸ ãƒœãƒ¼ãƒŠã‚¹è¨ˆç®—: Prime={prime_bonus if prime_info.get('is_prime') else 0}, é…é€={shipping_bonus if ship_hours and ship_hours <= 24 else 0}, ãƒ–ãƒ©ãƒ³ãƒ‰={brand_bonus if prime_info.get('seller_type') in ['official', 'authorized'] else 0}, ç·åˆ={bonus}")
        return bonus
        
    except Exception as e:
        logger.error(f"âŒ Shopeeãƒœãƒ¼ãƒŠã‚¹è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e}")
        return 0  # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å€¤

def process_batch_with_shopee_optimization(df, title_column='clean_title', limit=20):
    """ Shopeeå‡ºå“æœ€é©åŒ–å‡¦ç†ï¼ˆPhase 4.0å¯¾å¿œç‰ˆï¼‰ """
    logger.info(f"ğŸš€ Shopeeæœ€é©åŒ–å‡¦ç†é–‹å§‹ï¼ˆPhase 4.0ï¼‰: {len(df) if df is not None else 0}ä»¶ (åˆ¶é™: {limit}ä»¶)")
    
    if df is None or df.empty: 
        logger.error("âŒ å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
        return pd.DataFrame()
    
    # ãƒãƒƒãƒå‡¦ç†åˆ¶é™ã®å–å¾—
    max_batch_size = get_config_value("system_settings", "batch_processing_limit", 100)
    if limit > max_batch_size:
        logger.warning(f"âš ï¸ æŒ‡å®šã•ã‚ŒãŸåˆ¶é™({limit})ãŒæœ€å¤§ãƒãƒƒãƒã‚µã‚¤ã‚º({max_batch_size})ã‚’è¶…ãˆã¦ã„ã¾ã™ã€‚æœ€å¤§ã‚µã‚¤ã‚ºã«èª¿æ•´ã—ã¾ã™ã€‚")
        limit = max_batch_size
    
    df_to_process = df.head(limit).copy() if limit and limit > 0 else df.copy()
    logger.info(f"ğŸ“Š å‡¦ç†å¯¾è±¡: {len(df_to_process)}ä»¶")
    
    # ãƒ–ãƒ©ãƒ³ãƒ‰è¾æ›¸ã®èª­ã¿è¾¼ã¿
    logger.info("ğŸ“š ãƒ–ãƒ©ãƒ³ãƒ‰è¾æ›¸èª­ã¿è¾¼ã¿ä¸­...")
    try:
        brand_dict = load_brand_dict()
        logger.info(f"âœ… ãƒ–ãƒ©ãƒ³ãƒ‰è¾æ›¸èª­ã¿è¾¼ã¿æˆåŠŸ: {len(brand_dict)}ä»¶")
    except Exception as e_brand:
        logger.error(f"âš ï¸ ãƒ–ãƒ©ãƒ³ãƒ‰è¾æ›¸èª­ã¿è¾¼ã¿å¤±æ•—: {e_brand}. ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¾æ›¸ä½¿ç”¨")
        brand_dict = {"FANCL": ["ãƒ•ã‚¡ãƒ³ã‚±ãƒ«"], "ORBIS": ["ã‚ªãƒ«ãƒ“ã‚¹"]}
    
    results = []
    success_count = 0
    error_count = 0
    retry_count = 0
    
    # ã‚¨ãƒ©ãƒ¼å‡¦ç†çµ±è¨ˆ
    error_stats = {
        'title_empty': 0,
        'brand_extraction_failed': 0,
        'llm_failed': 0,
        'prime_info_failed': 0,
        'scoring_failed': 0,
        'general_error': 0
    }
    
    # æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ã®å–å¾—
    max_retries = get_config_value("system_settings", "max_retry_attempts", 2)
    logger.info(f"ğŸ”„ ãƒãƒƒãƒå‡¦ç†é–‹å§‹... (æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°: {max_retries})")

    for idx, row_series in df_to_process.iterrows():
        row_processing_success = False
        retry_attempts = 0
        
        while not row_processing_success and retry_attempts <= max_retries:
            try:
                current_row_dict = row_series.to_dict() 
                logger.info(f"\nğŸ” å‡¦ç†ä¸­ {success_count + error_count + 1}/{len(df_to_process)}: {current_row_dict.get(title_column, 'N/A')[:50]}...")
                if retry_attempts > 0:
                    logger.info(f"  ğŸ”„ ãƒªãƒˆãƒ©ã‚¤ {retry_attempts}/{max_retries}")
                    retry_count += 1

                # Step 1: ã‚¿ã‚¤ãƒˆãƒ«æ¤œè¨¼
                clean_title_val = str(current_row_dict.get(title_column, '')).strip()
                if not clean_title_val or clean_title_val.lower() == 'nan':
                    logger.warning(f"âš ï¸ è¡Œ{idx}: å•†å“åãŒç©ºã¾ãŸã¯ç„¡åŠ¹ã€‚")
                    error_stats['title_empty'] += 1
                    error_details = {
                        'search_status': 'error', 
                        'error_reason': 'å•†å“åãŒç©ºã¾ãŸã¯ç„¡åŠ¹', 
                        'data_source': 'å…¥åŠ›ã‚¨ãƒ©ãƒ¼',
                        'error_category': 'title_empty',
                        'retry_attempted': retry_attempts
                    }
                    results.append({**current_row_dict, **error_details})
                    error_count += 1
                    break  # ãƒªãƒˆãƒ©ã‚¤ä¸è¦ãªã‚¨ãƒ©ãƒ¼

                # Step 2: ãƒ–ãƒ©ãƒ³ãƒ‰ãƒ»æ•°é‡æŠ½å‡º
                try:
                    extracted_info_val = extract_brand_and_quantity(clean_title_val, brand_dict)
                    brand_name_val = extracted_info_val[0]
                    cleaned_text_for_title_val = clean_title_val
                    logger.info(f"  ğŸ·ï¸ ãƒ–ãƒ©ãƒ³ãƒ‰æ¤œå‡ºå¾Œ: brand_name='{brand_name_val}'")
                except Exception as e_brand_extract:
                    logger.error(f"  âš ï¸ ãƒ–ãƒ©ãƒ³ãƒ‰æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e_brand_extract}")
                    error_stats['brand_extraction_failed'] += 1
                    # ãƒ–ãƒ©ãƒ³ãƒ‰æŠ½å‡ºå¤±æ•—ã§ã‚‚å‡¦ç†ç¶šè¡Œ
                    brand_name_val = ''
                    cleaned_text_for_title_val = clean_title_val

                # Step 3: æ—¥æœ¬èªåŒ–
                japanese_name_val = clean_title_val  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                llm_source_val = "Original"
                try:
                    japanese_name_val, llm_source_val = get_japanese_name_hybrid(cleaned_text_for_title_val)
                    logger.info(f"  ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªåŒ–å¾Œ: japanese_name='{japanese_name_val}', llm_source='{llm_source_val}'")
                except Exception as e_llm:
                    logger.error(f"  âš ï¸ LLMæ—¥æœ¬èªåŒ–ã‚¨ãƒ©ãƒ¼: {e_llm}. å…ƒã‚¿ã‚¤ãƒˆãƒ«ä½¿ç”¨")
                    error_stats['llm_failed'] += 1
                    japanese_name_val = cleaned_text_for_title_val
                    llm_source_val = "Error_Fallback"

                # Step 4: ASINæ±ºå®š
                asin_val = current_row_dict.get('asin', current_row_dict.get('amazon_asin', f"GEN{str(success_count + error_count + 1).zfill(9)}V8K"))
                logger.info(f"  ğŸ” ASINæ±ºå®š: '{asin_val}'")

                # Step 5: Primeæƒ…å ±å–å¾—
                prime_info_val = {}
                try:
                    prime_info_val = get_prime_and_seller_info_v8_enhanced(asin=asin_val, brand_name=brand_name_val)
                    logger.info(f"  ğŸ¯ prime_infoå–å¾—å¾Œ: is_prime={prime_info_val.get('is_prime')}, seller_type='{prime_info_val.get('seller_type')}', ship_hours={prime_info_val.get('ship_hours')}, prime_confidence_score={prime_info_val.get('prime_confidence')}")
                except Exception as e_prime:
                    logger.error(f"  âš ï¸ Primeæƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e_prime}. ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æƒ…å ±ä½¿ç”¨")
                    error_stats['prime_info_failed'] += 1
                    prime_info_val = {
                        'is_prime': False, 'seller_name': 'ã‚¨ãƒ©ãƒ¼æ™‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡ºå“è€…', 'seller_type': 'unknown',
                        'ship_hours': None, 'prime_confidence': 0.0, 'prime_reason': f'Error: {str(e_prime)}'
                    }

                # Step 6: Shopeeé©æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—
                shopee_score_val = 30  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å€¤
                try:
                    shopee_score_val = calculate_shopee_suitability_score(current_row_dict, prime_info_val)
                    logger.info(f"  ğŸ“Š Shopeeã‚¹ã‚³ã‚¢è¨ˆç®—å¾Œ: shopee_score={shopee_score_val}")
                except Exception as e_score:
                    logger.error(f"  âš ï¸ Shopeeã‚¹ã‚³ã‚¢è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {e_score}. ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å€¤ä½¿ç”¨")
                    error_stats['scoring_failed'] += 1
                    shopee_score_val = 30

                # Step 7: é–¢é€£æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—
                relevance_score_val = calculate_relevance_score(cleaned_text_for_title_val, japanese_name_val)
                match_percentage_val = calculate_match_percentage(cleaned_text_for_title_val, japanese_name_val)
                
                # Step 8: çµæœãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰
                result_data_to_update = {
                    'clean_title': cleaned_text_for_title_val,
                    'japanese_name': japanese_name_val,
                    'amazon_title': japanese_name_val,
                    'asin': asin_val,
                    'amazon_asin': asin_val,
                    'amazon_brand': brand_name_val,
                    'extracted_brand': brand_name_val,
                    'extracted_quantity': extracted_info_val[1],
                    'llm_source': llm_source_val,
                    'is_prime': prime_info_val.get('is_prime'),
                    'seller_name': prime_info_val.get('seller_name'),
                    'seller_type': prime_info_val.get('seller_type'),
                    'seller_id': prime_info_val.get('seller_id'),
                    'is_amazon_seller': prime_info_val.get('is_amazon_seller'),
                    'is_official_seller': prime_info_val.get('is_official_seller'),
                    'is_fba': prime_info_val.get('is_fba'),
                    'ship_hours': prime_info_val.get('ship_hours'),
                    'ship_confidence': prime_info_val.get('ship_confidence'),
                    'ship_bucket': prime_info_val.get('ship_bucket'),
                    'ship_category': prime_info_val.get('ship_category'),
                    'ship_source': prime_info_val.get('ship_source'),
                    'ship_priority': prime_info_val.get('ship_priority'),
                    'prime_confidence': prime_info_val.get('prime_confidence'),
                    'prime_reason': prime_info_val.get('prime_reason'),
                    'shopee_suitability_score': shopee_score_val,
                    'relevance_score': relevance_score_val,
                    'match_percentage': match_percentage_val,
                    'category': prime_info_val.get('category'),
                    'classification_reason': prime_info_val.get('classification_reason'),
                    'classification_confidence': prime_info_val.get('classification_confidence'),
                    'shopee_score_bonus': prime_info_val.get('shopee_score_bonus'),
                    'search_status': 'success',
                    'api_source': 'v8_enhanced',
                    'data_source': 'v8_ERROR_HANDLING_ENHANCED',
                    'v8_system_used': 'Phase 4.0',
                    'retry_attempted': retry_attempts,
                    'processing_notes': f"æˆåŠŸï¼ˆãƒªãƒˆãƒ©ã‚¤{retry_attempts}å›ï¼‰" if retry_attempts > 0 else "æˆåŠŸ"
                }
                
                final_result_row = current_row_dict.copy()
                final_result_row.update(result_data_to_update) 
                
                results.append(final_result_row)
                success_count += 1
                row_processing_success = True  # æˆåŠŸãƒ•ãƒ©ã‚°
                
                logger.info(f"   âœ… æˆåŠŸ: ASIN={asin_val}, Prime={final_result_row.get('is_prime')}, ShipH={final_result_row.get('ship_hours')}, PrimeScore={final_result_row.get('prime_confidence')}, ShopeeScore={shopee_score_val}")

            except Exception as e_main_loop:
                retry_attempts += 1
                logger.error(f"âŒ è¡Œ{idx} ã§ä¸€èˆ¬ã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ{retry_attempts}/{max_retries+1}): {str(e_main_loop)}")
                
                if retry_attempts > max_retries:
                    # æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ã«é”ã—ãŸå ´åˆ
                    logger.error(f"ğŸ’¥ è¡Œ{idx} æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°è¶…éã€‚ã‚¨ãƒ©ãƒ¼ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦è¨˜éŒ²ã€‚")
                    error_count += 1
                    error_stats['general_error'] += 1
                    error_details_loop = {
                        'search_status': 'error', 
                        'error_reason': f"æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å¾Œã‚¨ãƒ©ãƒ¼: {str(e_main_loop)[:150]}", 
                        'data_source': 'å‡¦ç†ãƒ«ãƒ¼ãƒ—å†…ã‚¨ãƒ©ãƒ¼',
                        'error_category': 'general_error',
                        'retry_attempted': retry_attempts-1,
                        'processing_notes': f"å¤±æ•—ï¼ˆ{retry_attempts-1}å›ãƒªãƒˆãƒ©ã‚¤å¾Œï¼‰"
                    }
                    results.append({**current_row_dict, **error_details_loop})
                    break
                else:
                    # ãƒªãƒˆãƒ©ã‚¤ç¶™ç¶š
                    logger.info(f"  ğŸ”„ {retry_attempts}ç§’å¾…æ©Ÿå¾Œãƒªãƒˆãƒ©ã‚¤...")
                    time.sleep(retry_attempts)  # æ®µéšçš„å¾…æ©Ÿæ™‚é–“
            
    # å‡¦ç†çµæœã‚µãƒãƒªãƒ¼
    logger.info(f"\nğŸ“Š ãƒãƒƒãƒå‡¦ç†å®Œäº†:")
    logger.info(f"  âœ… æˆåŠŸ: {success_count}ä»¶")
    logger.info(f"  âŒ ã‚¨ãƒ©ãƒ¼: {error_count}ä»¶")
    logger.info(f"  ğŸ”„ ãƒªãƒˆãƒ©ã‚¤å®Ÿè¡Œ: {retry_count}å›")
    
    logger.info(f"\nğŸ“‹ ã‚¨ãƒ©ãƒ¼å†…è¨³:")
    for error_type, count in error_stats.items():
        if count > 0:
            logger.info(f"  {error_type}: {count}ä»¶")
    
    if results:
        result_df = pd.DataFrame(results)
        logger.info(f"ğŸ“‹ çµæœDFã‚«ãƒ©ãƒ æ•°: {len(result_df.columns)}")
        return result_df
    else:
        logger.error("âŒ å‡¦ç†çµæœãŒç©ºã§ã™ã€‚")
        error_cols = ['search_status', 'error_reason', 'data_source', 'error_category', 'retry_attempted']
        df_cols = df.columns.tolist() if df is not None else []
        return pd.DataFrame(columns=list(set(df_cols + error_cols)))

def calculate_relevance_score(original, japanese):
    """é–¢é€£æ€§ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆç¾å®¹ç”¨èªè¾æ›¸å¯¾å¿œç‰ˆï¼‰"""
    if not original or not japanese: 
        return 0
    
    # ç¾å®¹ç”¨èªè¾æ›¸ã®èª­ã¿è¾¼ã¿
    beauty_terms = load_beauty_terms_dict()
    
    # è‹±èªã®å˜èªã‚’å°æ–‡å­—ã«ã€æ—¥æœ¬èªã¯å…ƒã®ã¾ã¾
    original_words = set(original.lower().split())
    japanese_words = set(japanese.split())
    
    if len(original_words) == 0:
        return 0
    
    # åŸºæœ¬çš„ãªèªã®ä¸€è‡´åº¦è¨ˆç®—
    direct_common_words = original_words & japanese_words
    basic_similarity = len(direct_common_words) / len(original_words)
    
    # ç¾å®¹ç”¨èªãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é«˜åº¦ä¸€è‡´åˆ¤å®š
    beauty_matches = 0
    total_beauty_terms = 0
    
    for eng_word in original_words:
        if eng_word in beauty_terms:
            total_beauty_terms += 1
            jp_variations = beauty_terms[eng_word]
            
            # æ—¥æœ¬èªã®å„ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã¨ã®ä¸€è‡´ãƒã‚§ãƒƒã‚¯
            for jp_variation in jp_variations:
                if jp_variation in japanese:
                    beauty_matches += 1
                    print(f"    âœ… ç¾å®¹ç”¨èªä¸€è‡´: '{eng_word}' â†’ '{jp_variation}'")
                    break
    
    # ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆç¾å®¹ç”¨èªé‡è¦–ï¼‰
    if total_beauty_terms > 0:
        # ç¾å®¹ç”¨èªãŒã‚ã‚‹å ´åˆ: ç¾å®¹ç”¨èªä¸€è‡´åº¦ã‚’é‡è¦–
        beauty_similarity = beauty_matches / total_beauty_terms
        # åŸºæœ¬ä¸€è‡´åº¦ã¨ç¾å®¹ç”¨èªä¸€è‡´åº¦ã®åŠ é‡å¹³å‡
        enhanced_similarity = (basic_similarity * 0.3) + (beauty_similarity * 0.7)
        print(f"    ğŸ“Š ç¾å®¹ç”¨èªå¼·åŒ–: åŸºæœ¬{basic_similarity:.2f} + ç¾å®¹{beauty_similarity:.2f} = {enhanced_similarity:.2f}")
    else:
        # ç¾å®¹ç”¨èªãŒãªã„å ´åˆ: åŸºæœ¬ä¸€è‡´åº¦ã®ã¿
        enhanced_similarity = basic_similarity
        print(f"    ğŸ“Š åŸºæœ¬ä¸€è‡´åº¦ã®ã¿: {enhanced_similarity:.2f}")
    
    similarity_score = int(enhanced_similarity * 100)
    
    # ä¿®æ­£: ã‚ªãƒ•ã‚»ãƒƒãƒˆãªã—ã®ç´”ç²‹ãªä¸€è‡´åº¦ã‚’è¿”ã™
    final_score = min(max(similarity_score, 0), 100)
    print(f"    ğŸ¯ æœ€çµ‚relevance_score: {final_score}%")
    
    return final_score

def calculate_match_percentage(original, japanese):
    """ä¸€è‡´ç‡è¨ˆç®—ï¼ˆç¾å®¹ç”¨èªè¾æ›¸å¯¾å¿œç‰ˆï¼‰"""
    if not original or not japanese:
        return 0
    
    # ç¾å®¹ç”¨èªè¾æ›¸ã®èª­ã¿è¾¼ã¿
    beauty_terms = load_beauty_terms_dict()
    
    # è‹±èªã®å˜èªã‚’å°æ–‡å­—ã«ã€æ—¥æœ¬èªã¯å…ƒã®ã¾ã¾
    original_words = set(original.lower().split())
    japanese_words = set(japanese.split())
    
    if len(original_words) == 0:
        return 0
    
    # åŸºæœ¬çš„ãªèªã®ä¸€è‡´åº¦è¨ˆç®—
    direct_common_words = original_words & japanese_words
    basic_similarity = len(direct_common_words) / len(original_words)
    
    # ç¾å®¹ç”¨èªãƒãƒƒãƒ”ãƒ³ã‚°ã«ã‚ˆã‚‹é«˜åº¦ä¸€è‡´åˆ¤å®š
    beauty_matches = 0
    total_beauty_terms = 0
    
    for eng_word in original_words:
        if eng_word in beauty_terms:
            total_beauty_terms += 1
            jp_variations = beauty_terms[eng_word]
            
            # æ—¥æœ¬èªã®å„ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã¨ã®ä¸€è‡´ãƒã‚§ãƒƒã‚¯
            for jp_variation in jp_variations:
                if jp_variation in japanese:
                    beauty_matches += 1
                    print(f"    âœ… ç¾å®¹ç”¨èªä¸€è‡´: '{eng_word}' â†’ '{jp_variation}'")
                    break
    
    # ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆç¾å®¹ç”¨èªé‡è¦–ï¼‰
    if total_beauty_terms > 0:
        # ç¾å®¹ç”¨èªãŒã‚ã‚‹å ´åˆ: ç¾å®¹ç”¨èªä¸€è‡´åº¦ã‚’é‡è¦–
        beauty_similarity = beauty_matches / total_beauty_terms
        # åŸºæœ¬ä¸€è‡´åº¦ã¨ç¾å®¹ç”¨èªä¸€è‡´åº¦ã®åŠ é‡å¹³å‡
        enhanced_similarity = (basic_similarity * 0.3) + (beauty_similarity * 0.7)
        print(f"    ğŸ“Š ç¾å®¹ç”¨èªå¼·åŒ–: åŸºæœ¬{basic_similarity:.2f} + ç¾å®¹{beauty_similarity:.2f} = {enhanced_similarity:.2f}")
    else:
        # ç¾å®¹ç”¨èªãŒãªã„å ´åˆ: åŸºæœ¬ä¸€è‡´åº¦ã®ã¿
        enhanced_similarity = basic_similarity
        print(f"    ğŸ“Š åŸºæœ¬ä¸€è‡´åº¦ã®ã¿: {enhanced_similarity:.2f}")
    
    similarity_score = int(enhanced_similarity * 100)
    
    # ä¿®æ­£: ã‚ªãƒ•ã‚»ãƒƒãƒˆãªã—ã®ç´”ç²‹ãªä¸€è‡´åº¦ã‚’è¿”ã™
    final_score = min(max(similarity_score, 0), 100)
    print(f"    ğŸ¯ æœ€çµ‚match_percentage: {final_score}%")
    
    return final_score

def analyze_beauty_terms_coverage(text):
    """ç¾å®¹ç”¨èªã®ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æ"""
    if not text:
        return {
            'total_terms': 0,
            'matched_terms': 0,
            'coverage_percentage': 0,
            'matched_details': []
        }
    
    # ç¾å®¹ç”¨èªè¾æ›¸ã®èª­ã¿è¾¼ã¿
    beauty_terms = load_beauty_terms_dict()
    
    # ãƒ†ã‚­ã‚¹ãƒˆã‚’å˜èªã«åˆ†å‰²
    words = set(text.lower().split())
    
    # ç¾å®¹ç”¨èªã®ãƒãƒƒãƒãƒ³ã‚°åˆ†æ
    matched_terms = []
    for eng_word in words:
        if eng_word in beauty_terms:
            jp_variations = beauty_terms[eng_word]
            matched_terms.append({
                'english': eng_word,
                'japanese_variations': jp_variations
            })
    
    # çµæœã®é›†è¨ˆ
    total_terms = len(beauty_terms)
    matched_count = len(matched_terms)
    coverage = (matched_count / total_terms * 100) if total_terms > 0 else 0
    
    return {
        'total_terms': total_terms,
        'matched_terms': matched_count,
        'coverage_percentage': round(coverage, 2),
        'matched_details': matched_terms
    }

def get_seller_info(asin):
    """ å‡ºå“è€…æƒ…å ±ã®å–å¾—ï¼ˆPhase 4.0å¯¾å¿œç‰ˆï¼‰ """
    logger.info(f"ğŸ” å‡ºå“è€…æƒ…å ±å–å¾—é–‹å§‹: ASIN={asin}")
    
    try:
        # è¨­å®šå€¤ã®å–å¾—
        api_timeout = get_config_value("api_settings", "timeout", 30)
        max_retries = get_config_value("api_settings", "max_retries", 3)
        
        # APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å®Ÿè¡Œ
        for attempt in range(max_retries):
            try:
                # TODO: å®Ÿéš›ã®APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å®Ÿè£…
                # ç¾åœ¨ã¯ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
                seller_types = ['amazon', 'official', 'authorized', 'verified', 'third_party', 'unknown']
                seller_type = np.random.choice(seller_types, p=[0.3, 0.2, 0.2, 0.1, 0.15, 0.05])
                
                seller_info = {
                    'seller_name': '',
                    'seller_type': seller_type,
                    'seller_id': f"SID_{np.random.randint(1000, 9999)}",
                    'is_amazon_seller': seller_type == 'amazon',
                    'is_official_seller': seller_type in ['official', 'authorized'],
                    'is_fba': np.random.choice([True, False], p=[0.7, 0.3])
                }
                
                if seller_type == 'amazon':
                    seller_info['seller_name'] = 'Amazon.co.jp'
                elif seller_type in ['official', 'authorized']:
                    seller_info['seller_name'] = f'å…¬å¼å‡ºå“è€…{np.random.randint(1, 100)}'
                else:
                    seller_info['seller_name'] = f'å‡ºå“è€…{np.random.randint(1, 1000)}'
                
                logger.info(f"âœ… å‡ºå“è€…æƒ…å ±å–å¾—æˆåŠŸ: {seller_info['seller_name']} ({seller_info['seller_type']})")
                return seller_info
                
            except Exception as e_api:
                if attempt < max_retries - 1:
                    logger.warning(f"âš ï¸ APIãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤±æ•— (è©¦è¡Œ{attempt + 1}/{max_retries}): {e_api}")
                    time.sleep(attempt + 1)  # æ®µéšçš„å¾…æ©Ÿ
                else:
                    raise e_api
        
    except Exception as e:
        logger.error(f"âŒ å‡ºå“è€…æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def get_shipping_info(asin):
    """ é…é€æƒ…å ±ã®å–å¾—ï¼ˆPhase 4.0å¯¾å¿œç‰ˆï¼‰ """
    logger.info(f"ğŸ” é…é€æƒ…å ±å–å¾—é–‹å§‹: ASIN={asin}")
    
    try:
        # è¨­å®šå€¤ã®å–å¾—
        api_timeout = get_config_value("api_settings", "timeout", 30)
        max_retries = get_config_value("api_settings", "max_retries", 3)
        
        # APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å®Ÿè¡Œ
        for attempt in range(max_retries):
            try:
                # TODO: å®Ÿéš›ã®APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å®Ÿè£…
                # ç¾åœ¨ã¯ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
                ship_hours = np.random.choice([12, 18, 24, 36, 48, 72, 96, None], p=[0.1, 0.2, 0.3, 0.15, 0.1, 0.05, 0.05, 0.05])
                
                if ship_hours is None:
                    ship_info = {
                        'ship_hours': None,
                        'ship_confidence': 0,
                        'ship_bucket': 'ä¸æ˜',
                        'ship_category': 'C',
                        'ship_source': 'unknown',
                        'ship_priority': 99
                    }
                else:
                    if ship_hours <= 12:
                        bucket, category, priority = "12hä»¥å†…", "è¶…é«˜é€Ÿ", 1
                    elif ship_hours <= 24:
                        bucket, category, priority = "24hä»¥å†…", "é«˜é€Ÿ", 2
                    elif ship_hours <= 48:
                        bucket, category, priority = "48hä»¥å†…", "æ¨™æº–", 3
                    elif ship_hours <= 72:
                        bucket, category, priority = "72hä»¥å†…", "ã‚„ã‚„ä½é€Ÿ", 4
                    else:
                        bucket, category, priority = "72hè¶…", "ä½é€Ÿ", 5
                    
                    ship_info = {
                        'ship_hours': ship_hours,
                        'ship_confidence': np.random.randint(50, 100),
                        'ship_bucket': bucket,
                        'ship_category': category,
                        'ship_source': 'APIå–å¾—',
                        'ship_priority': priority
                    }
                
                logger.info(f"âœ… é…é€æƒ…å ±å–å¾—æˆåŠŸ: {ship_info['ship_hours']}æ™‚é–“ ({ship_info['ship_category']})")
                return ship_info
                
            except Exception as e_api:
                if attempt < max_retries - 1:
                    logger.warning(f"âš ï¸ APIãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤±æ•— (è©¦è¡Œ{attempt + 1}/{max_retries}): {e_api}")
                    time.sleep(attempt + 1)  # æ®µéšçš„å¾…æ©Ÿ
                else:
                    raise e_api
        
    except Exception as e:
        logger.error(f"âŒ é…é€æƒ…å ±å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return None