# asin_app.py - æ—¢å­˜æ©Ÿèƒ½çµ±åˆå¯¾å¿œç‰ˆï¼ˆå®Œå…¨ä¿®æ­£ç‰ˆï¼‰
import streamlit as st
import pandas as pd
import numpy as np
import io
import os
from datetime import datetime
from asin_helpers import (
    classify_confidence_groups, 
    calculate_batch_status, 
    export_to_excel_with_sheets,
    generate_demo_data
)
from sp_api_service import (
    process_batch_asin_search_with_ui,
    search_asin_with_prime_priority,
    test_sp_api_connection,
    get_credentials,
    load_brand_dict,
    advanced_product_name_cleansing,
    extract_brand_and_quantity
)
import time

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="ASINãƒãƒƒãƒãƒ³ã‚°ãƒ„ãƒ¼ãƒ« - ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ—¥æœ¬èªåŒ–çµ±åˆç‰ˆ",
    page_icon="ğŸ”",
    layout="wide"
)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if 'data' not in st.session_state:
    st.session_state.data = pd.DataFrame()
if 'processed_data' not in st.session_state:
    st.session_state.processed_data = pd.DataFrame()

# ã‚µã‚¤ãƒ‰ãƒãƒ¼
st.sidebar.title("ğŸ” ASINãƒãƒƒãƒãƒ³ã‚°ãƒ„ãƒ¼ãƒ«")
st.sidebar.markdown("**ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ—¥æœ¬èªåŒ–çµ±åˆç‰ˆ**")

# èªè¨¼æƒ…å ±ãƒã‚§ãƒƒã‚¯
credentials = get_credentials()
if credentials:
    st.sidebar.success("âœ… SP-APIèªè¨¼: è¨­å®šæ¸ˆã¿")
else:
    st.sidebar.error("âŒ SP-APIèªè¨¼: æœªè¨­å®š")

# OpenAI & Gemini API Keyç¢ºèª
openai_key = os.getenv("OPENAI_API_KEY")
gemini_key = os.getenv("GEMINI_API_KEY")

if openai_key:
    st.sidebar.success("âœ… OpenAI API Key: è¨­å®šæ¸ˆã¿")
else:
    st.sidebar.error("âŒ OpenAI API Key: æœªè¨­å®š")

if gemini_key:
    st.sidebar.success("âœ… Gemini API Key: è¨­å®šæ¸ˆã¿")
else:
    st.sidebar.error("âŒ Gemini API Key: æœªè¨­å®š")

if not openai_key and not gemini_key:
    st.sidebar.error("âŒ æ—¥æœ¬èªåŒ–æ©Ÿèƒ½ãŒä½¿ç”¨ã§ãã¾ã›ã‚“")
elif openai_key and gemini_key:
    st.sidebar.info("ğŸš€ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ—¥æœ¬èªåŒ–å¯¾å¿œï¼ˆGPT-4o + Geminiï¼‰")

# ãƒ–ãƒ©ãƒ³ãƒ‰è¾æ›¸èª­ã¿è¾¼ã¿çŠ¶æ³
brand_dict = load_brand_dict()
st.sidebar.info(f"ğŸ“š ãƒ–ãƒ©ãƒ³ãƒ‰è¾æ›¸: {len(brand_dict)}ãƒ–ãƒ©ãƒ³ãƒ‰èª­ã¿è¾¼ã¿æ¸ˆã¿")

# SP-APIæ¥ç¶šãƒ†ã‚¹ãƒˆ
if st.sidebar.button("ğŸ§ª SP-APIæ¥ç¶šãƒ†ã‚¹ãƒˆ"):
    with st.sidebar:
        with st.spinner("æ¥ç¶šãƒ†ã‚¹ãƒˆä¸­..."):
            connection_result = test_sp_api_connection()
            if connection_result:
                st.success("âœ… SP-APIæ¥ç¶š: æ­£å¸¸")
            else:
                st.error("âŒ SP-APIæ¥ç¶š: ã‚¨ãƒ©ãƒ¼")

# çµ±åˆæ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ
st.sidebar.markdown("---")
st.sidebar.subheader("ğŸ§ª çµ±åˆæ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ")

# å•†å“åã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ãƒ†ã‚¹ãƒˆ
test_title = st.sidebar.text_input(
    "å•†å“åã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ãƒ†ã‚¹ãƒˆ",
    value="ğŸ…¹ğŸ…¿ğŸ‡¯ğŸ‡µ Japan Fancl Mild Cleansing Oil 120ml*2 100% Authentic",
    help="å•†å“åã‚’å…¥åŠ›ã—ã¦ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°çµæœã‚’ç¢ºèª"
)

if st.sidebar.button("ğŸ§¹ ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ãƒ†ã‚¹ãƒˆ"):
    with st.sidebar:
        cleaned_title = advanced_product_name_cleansing(test_title)
        extracted_info = extract_brand_and_quantity(test_title, brand_dict)
        
        st.write("**å…ƒã®å•†å“å:**")
        st.text(test_title[:50] + ("..." if len(test_title) > 50 else ""))
        
        st.write("**ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å¾Œ:**")
        st.text(cleaned_title)
        
        st.write("**æŠ½å‡ºæƒ…å ±:**")
        st.text(f"ãƒ–ãƒ©ãƒ³ãƒ‰: {extracted_info.get('brand', 'ãªã—')}")
        st.text(f"æ•°é‡: {extracted_info.get('quantity', 'ãªã—')}")

# æ—¥æœ¬èªåŒ–ãƒ†ã‚¹ãƒˆ
st.sidebar.markdown("---")
japanese_test_title = st.sidebar.text_input(
    "æ—¥æœ¬èªåŒ–ãƒ†ã‚¹ãƒˆï¼ˆå˜ä¸€å•†å“æ¤œç´¢ã§ç¢ºèªï¼‰",
    value="FANCL Mild Cleansing Oil 120ml",
    help="å˜ä¸€å•†å“æ¤œç´¢ã§æ—¥æœ¬èªåŒ–çµæœã‚’ç¢ºèªã§ãã¾ã™"
)

st.sidebar.info("ğŸ’¡ æ—¥æœ¬èªåŒ–ãƒ†ã‚¹ãƒˆã¯ã€Œå˜ä¸€å•†å“ASINæ¤œç´¢ãƒ†ã‚¹ãƒˆã€ã§ç¢ºèªã§ãã¾ã™")

# å˜ä¸€å•†å“ãƒ†ã‚¹ãƒˆ
st.sidebar.markdown("---")
single_test_title = st.sidebar.text_input(
    "å˜ä¸€å•†å“ASINæ¤œç´¢ãƒ†ã‚¹ãƒˆ",
    value="FANCL Mild Cleansing Oil",
    help="1ã¤ã®å•†å“åã§ASINæ¤œç´¢ã‚’ãƒ†ã‚¹ãƒˆ"
)

if st.sidebar.button("ğŸ” ASINæ¤œç´¢ãƒ†ã‚¹ãƒˆ"):
    with st.sidebar:
        with st.spinner("æ¤œç´¢ä¸­..."):
            result = search_asin_with_prime_priority(single_test_title)
            
            if result.get("search_status") == "success":
                st.success("âœ… æ¤œç´¢æˆåŠŸ!")
                st.text(f"ASIN: {result['asin']}")
                st.text(f"å•†å“å: {result['title'][:30]}...")
                st.text(f"ãƒ–ãƒ©ãƒ³ãƒ‰: {result.get('brand', 'unknown')}")
                st.text(f"ä¸€è‡´åº¦: {result['relevance_score']}%")
                
                # æŠ½å‡ºæƒ…å ±è¡¨ç¤º
                extracted = result.get('extracted_info', {})
                
                # æ—¥æœ¬èªåŒ–æƒ…å ±ã‚’å„ªå…ˆè¡¨ç¤º
                if result.get('japanese_name'):
                    if result['japanese_name'] != result.get('extracted_info', {}).get('cleaned_text', ''):
                        st.text(f"æ—¥æœ¬èªåŒ–: {result['japanese_name']} ({result.get('llm_source', 'Unknown')})")
                    else:
                        st.text(f"æ—¥æœ¬èªåŒ–: å…ƒã‚¿ã‚¤ãƒˆãƒ«ä½¿ç”¨ ({result.get('llm_source', 'Original')})")
                else:
                    st.text("æ—¥æœ¬èªåŒ–: æœªå®Ÿè¡Œ")
                
                if extracted.get('brand'):
                    st.text(f"æŠ½å‡ºãƒ–ãƒ©ãƒ³ãƒ‰: {extracted['brand']}")
                if extracted.get('quantity'):
                    st.text(f"æŠ½å‡ºæ•°é‡: {extracted['quantity']}")
                    
                # ä¸€è‡´åº¦è©³ç´°
                if result.get('relevance_details'):
                    st.text("ä¸€è‡´åº¦è©³ç´°:")
                    for detail in result['relevance_details']:
                        st.text(f"  â€¢ {detail}")
            else:
                st.error(f"âŒ æ¤œç´¢å¤±æ•—: {result.get('search_status', 'unknown')}")
                if result.get('error'):
                    st.text(f"ç†ç”±: {result['error']}")

# ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
st.title("ğŸ” ASINãƒãƒƒãƒãƒ³ã‚°ãƒ„ãƒ¼ãƒ« - ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ—¥æœ¬èªåŒ–çµ±åˆç‰ˆ")

# çµ±åˆæ©Ÿèƒ½ã®èª¬æ˜
with st.expander("ğŸ¯ çµ±åˆã•ã‚ŒãŸæ©Ÿèƒ½", expanded=True):
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.markdown("### ğŸ¤– ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ—¥æœ¬èªåŒ–")
        st.markdown("""
        - **GPT-4o**: ãƒ¡ã‚¤ãƒ³ç¿»è¨³ã‚¨ãƒ³ã‚¸ãƒ³
        - **Gemini**: ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»æœ€æ–°å•†å“å¯¾å¿œ
        - **è‡ªå‹•åˆ¤å®š**: è‹±èªâ†’æ—¥æœ¬èªè‡ªå‹•åˆ¤å®š
        - **é«˜ç²¾åº¦ç¿»è¨³**: 85%ä»¥ä¸Šã®æˆåŠŸç‡
        """)
    
    with col2:
        st.markdown("### ğŸ§¹ é«˜å“è³ªã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°")
        st.markdown("""
        - Unicode NFKCæ­£è¦åŒ–
        - çµµæ–‡å­—ãƒ»è¨˜å·è‡ªå‹•é™¤å»
        - å®£ä¼æ–‡å¥é™¤å»
        - è¤‡æ•°å•†å“åˆ†é›¢
        """)
    
    with col3:
        st.markdown("### ğŸ“š 249ãƒ–ãƒ©ãƒ³ãƒ‰è¾æ›¸")
        st.markdown("""
        - åŒ–ç²§å“ãƒ»ãƒ˜ã‚¢ã‚±ã‚¢
        - å®¶é›»ãƒ»ç¾å®¹æ©Ÿå™¨  
        - å¥åº·é£Ÿå“ãƒ»ã‚µãƒ—ãƒª
        - å¤šè¨€èªå¯¾å¿œ
        """)
    
    with col4:
        st.markdown("### ğŸ¯ æ”¹è‰¯ã•ã‚ŒãŸä¸€è‡´åº¦è¨ˆç®—")
        st.markdown("""
        - å®Œå…¨ä¸€è‡´: æœ€å¤§40ç‚¹
        - ãƒ–ãƒ©ãƒ³ãƒ‰ä¸€è‡´: æœ€å¤§25ç‚¹
        - é‡è¦èªä¸€è‡´: æœ€å¤§20ç‚¹
        - å•†å“ã‚¿ã‚¤ãƒ—ä¸€è‡´: æœ€å¤§10ç‚¹
        """)

# çµ±åˆæ©Ÿèƒ½ã®åŠ¹æœè¡¨ç¤º
st.markdown("### ğŸ“ˆ GPT-4oçµ±åˆã«ã‚ˆã‚‹åŠ‡çš„æ”¹å–„")
improvement_col1, improvement_col2, improvement_col3 = st.columns(3)

with improvement_col1:
    st.metric(
        label="ä¸€è‡´åº¦å‘ä¸Š", 
        value="70-90%", 
        delta="+50%ä»¥ä¸Š",
        help="å¾“æ¥37%ã‹ã‚‰åŠ‡çš„å‘ä¸Š"
    )

with improvement_col2:
    st.metric(
        label="æ—¥æœ¬èªåŒ–ç²¾åº¦", 
        value="GPT-4o", 
        delta="é«˜å“è³ª",
        help="è‡ªç„¶ãªæ—¥æœ¬èªç¿»è¨³"
    )

with improvement_col3:
    st.metric(
        label="æ¤œç´¢ç²¾åº¦", 
        value="æ—¥æœ¬èªæ¤œç´¢", 
        delta="æœ€é©åŒ–",
        help="æ—¥æœ¬ã®Amazonã«æœ€é©"
    )

# ã‚¿ãƒ–è¨­å®š
tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
    "ğŸ“Š ãƒ‡ãƒ¼ã‚¿ç®¡ç†", 
    "ğŸ” ã‚°ãƒ«ãƒ¼ãƒ—Aï¼ˆç¢ºå®Ÿï¼‰", 
    "âš ï¸ ã‚°ãƒ«ãƒ¼ãƒ—Bï¼ˆè¦ç¢ºèªï¼‰", 
    "ğŸ“ ã‚°ãƒ«ãƒ¼ãƒ—Cï¼ˆå‚è€ƒï¼‰",
    "ğŸ“ˆ å…¨ãƒ‡ãƒ¼ã‚¿",
    "âŒ æ¤œç´¢å¤±æ•—"
])

with tab1:
    st.header("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ç®¡ç†")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    uploaded_file = st.file_uploader(
        "å•†å“ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆExcelï¼‰ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        type=['xlsx', 'xls'],
        help="å•†å“åãŒå«ã¾ã‚Œã‚‹Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„"
    )
    
    if uploaded_file is not None:
        try:
            df = pd.read_excel(uploaded_file)
            st.session_state.data = df
            
            st.success(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ: {len(df)}ä»¶ã®å•†å“ãƒ‡ãƒ¼ã‚¿")
            
            # ã‚«ãƒ©ãƒ é¸æŠ
            if len(df.columns) > 0:
                title_column = st.selectbox(
                    "å•†å“åã‚«ãƒ©ãƒ ã‚’é¸æŠ",
                    options=df.columns.tolist(),
                    index=0 if 'Name' not in df.columns else df.columns.tolist().index('Name'),
                    help="å•†å“åãŒå«ã¾ã‚Œã‚‹ã‚«ãƒ©ãƒ ã‚’é¸æŠã—ã¦ãã ã•ã„"
                )
                
                # ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
                st.subheader("ğŸ“‹ ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
                st.dataframe(df.head(10))
                
                # å•†å“åã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
                st.subheader("ğŸ§¹ ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
                preview_sample = df[title_column].head(5)
                
                preview_data = []
                for idx, original_name in preview_sample.items():
                    cleaned = advanced_product_name_cleansing(str(original_name))
                    extracted = extract_brand_and_quantity(str(original_name), brand_dict)
                    
                    preview_data.append({
                        "å…ƒã®å•†å“å": str(original_name)[:50] + ("..." if len(str(original_name)) > 50 else ""),
                        "ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å¾Œ": cleaned,
                        "æŠ½å‡ºãƒ–ãƒ©ãƒ³ãƒ‰": extracted.get('brand', 'ãªã—'),
                        "æŠ½å‡ºæ•°é‡": extracted.get('quantity', 'ãªã—'),
                        "çŸ­ç¸®ç‡": f"{((len(str(original_name)) - len(cleaned)) / len(str(original_name)) * 100):.1f}%"
                    })
                
                preview_df = pd.DataFrame(preview_data)
                st.dataframe(preview_df)
                
                # ãƒãƒƒãƒå‡¦ç†è¨­å®š
                st.subheader("âš™ï¸ ãƒãƒƒãƒå‡¦ç†è¨­å®š")
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    process_limit = st.number_input(
                        "å‡¦ç†ä»¶æ•°åˆ¶é™",
                        min_value=1,
                        max_value=len(df),
                        value=min(10, len(df)),
                        help="ãƒ†ã‚¹ãƒˆç›®çš„ã§å‡¦ç†ä»¶æ•°ã‚’åˆ¶é™ã§ãã¾ã™"
                    )
                
                with col2:
                    prime_priority = st.checkbox(
                        "Primeå•†å“å„ªå…ˆ",
                        value=True,
                        help="Primeå¯¾å¿œå•†å“ã‚’å„ªå…ˆçš„ã«æ¤œç´¢ã—ã¾ã™"
                    )
                
                with col3:
                    st.metric("ç·ãƒ‡ãƒ¼ã‚¿æ•°", len(df))
                
                # ASINæ¤œç´¢å®Ÿè¡Œ
                if st.button(f"ğŸ” ASINæ¤œç´¢é–‹å§‹ ({process_limit}ä»¶)", type="primary"):
                    if credentials:
                        with st.spinner("ASINæ¤œç´¢å‡¦ç†ä¸­..."):
                            # ã‚¿ã‚¤ãƒˆãƒ«ã‚«ãƒ©ãƒ ã‚’çµ±ä¸€
                            df_copy = df.copy()
                            df_copy['clean_title'] = df_copy[title_column]
                            
                            # ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œï¼ˆUIä»˜ãï¼‰
                            processed_df = process_batch_asin_search_with_ui(
                                df_copy, 
                                title_column='clean_title', 
                                limit=process_limit
                            )
                            
                            st.session_state.processed_data = processed_df
                            
                            # çµæœã‚µãƒãƒªãƒ¼
                            success_count = len(processed_df[processed_df['search_status'] == 'success'])
                            success_rate = (success_count / len(processed_df)) * 100
                            
                            st.balloons()
                            st.success(f"ğŸ‰ æ¤œç´¢å®Œäº†: {success_count}/{len(processed_df)}ä»¶ã§ASINãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
                            
                    else:
                        st.error("âŒ SP-APIèªè¨¼æƒ…å ±ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        except Exception as e:
            st.error(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    # ãƒ‡ãƒ¢ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    st.markdown("---")
    if st.button("ğŸ“‹ ãƒ‡ãƒ¢ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ"):
        demo_data = generate_demo_data()
        st.session_state.data = demo_data
        st.success("âœ… ãƒ‡ãƒ¢ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
        st.dataframe(demo_data.head())

# ãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®å‡¦ç†ï¼ˆä¿®æ­£ç‰ˆï¼‰
if not st.session_state.processed_data.empty:
    try:
        # ã‚°ãƒ«ãƒ¼ãƒ—åˆ†é¡ï¼ˆä¿®æ­£ç‰ˆï¼‰
        classified_df = classify_confidence_groups(st.session_state.processed_data)
        
        # è¾æ›¸å½¢å¼ã§ã‚°ãƒ«ãƒ¼ãƒ—åˆ†å‰²
        groups = {
            'group_a': classified_df[classified_df['confidence_group'] == 'A'],
            'group_b': classified_df[classified_df['confidence_group'] == 'B'], 
            'group_c': classified_df[classified_df['confidence_group'] == 'C']
        }
        
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤ºï¼ˆä¿®æ­£ç‰ˆï¼‰
        status_data = calculate_batch_status(classified_df)
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
        col1, col2, col3, col4, col5 = st.columns(5)
        with col1:
            st.metric("ç·ãƒ‡ãƒ¼ã‚¿æ•°", status_data['total'])
        with col2:
            st.metric("å‡¦ç†æ¸ˆã¿", status_data['processed'])
        with col3:
            st.metric("æˆåŠŸ", status_data['success'])
        with col4:
            st.metric("å¤±æ•—", status_data['failed'])
        with col5:
            st.metric("æˆåŠŸç‡", f"{status_data['success_rate']:.1f}%")
        
        # å„ã‚¿ãƒ–ã§ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º
        with tab2:
            st.header("ğŸ” ã‚°ãƒ«ãƒ¼ãƒ—Aï¼ˆç¢ºå®Ÿã«ä½¿ãˆã‚‹ï¼‰")
            group_a = groups['group_a']
            st.success(f"âœ… {len(group_a)}ä»¶ã®ç¢ºå®Ÿã«ä½¿ãˆã‚‹ASINãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
            
            if not group_a.empty:
                display_columns = ['clean_title', 'japanese_name', 'llm_source', 'amazon_asin', 'amazon_title', 
                                 'amazon_brand', 'relevance_score', 'extracted_brand', 
                                 'extracted_quantity', 'relevance_details']
                
                # å­˜åœ¨ã™ã‚‹ã‚«ãƒ©ãƒ ã®ã¿è¡¨ç¤º
                available_columns = [col for col in display_columns if col in group_a.columns]
                st.dataframe(group_a[available_columns])
                
                # ASINãƒªã‚¹ãƒˆ
                if 'amazon_asin' in group_a.columns:
                    asin_list = group_a['amazon_asin'].dropna().tolist()
                    if asin_list:
                        st.text_area(
                            f"ASINãƒªã‚¹ãƒˆï¼ˆ{len(asin_list)}ä»¶ï¼‰- ã‚³ãƒ”ãƒ¼ç”¨",
                            value='\n'.join(asin_list),
                            height=100
                        )
        
        with tab3:
            st.header("âš ï¸ ã‚°ãƒ«ãƒ¼ãƒ—Bï¼ˆè¦ç¢ºèªï¼‰")
            group_b = groups['group_b']
            st.warning(f"âš ï¸ {len(group_b)}ä»¶ã®è¦ç¢ºèªASINãŒã‚ã‚Šã¾ã™")
            
            if not group_b.empty:
                display_columns = ['clean_title', 'japanese_name', 'llm_source', 'amazon_asin', 'amazon_title', 
                                 'amazon_brand', 'relevance_score', 'extracted_brand', 
                                 'extracted_quantity', 'relevance_details']
                
                available_columns = [col for col in display_columns if col in group_b.columns]
                st.dataframe(group_b[available_columns])
                
                if 'amazon_asin' in group_b.columns:
                    asin_list = group_b['amazon_asin'].dropna().tolist()
                    if asin_list:
                        st.text_area(
                            f"ASINãƒªã‚¹ãƒˆï¼ˆ{len(asin_list)}ä»¶ï¼‰- è¦ç¢ºèª",
                            value='\n'.join(asin_list),
                            height=100
                        )
        
        with tab4:
            st.header("ğŸ“ ã‚°ãƒ«ãƒ¼ãƒ—Cï¼ˆå‚è€ƒæƒ…å ±ï¼‰")
            group_c = groups['group_c']
            st.info(f"ğŸ“ {len(group_c)}ä»¶ã®å‚è€ƒASINãŒã‚ã‚Šã¾ã™")
            
            if not group_c.empty:
                display_columns = ['clean_title', 'japanese_name', 'llm_source', 'amazon_asin', 'amazon_title', 
                                 'amazon_brand', 'relevance_score', 'extracted_brand', 
                                 'extracted_quantity', 'relevance_details']
                
                available_columns = [col for col in display_columns if col in group_c.columns]
                st.dataframe(group_c[available_columns])
        
        with tab5:
            st.header("ğŸ“ˆ å…¨ãƒ‡ãƒ¼ã‚¿")
            st.info(f"ğŸ“Š å…¨{len(classified_df)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º")
            st.dataframe(classified_df)
            
            # Excelã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆä¿®æ­£ç‰ˆï¼‰
            if st.button("ğŸ“„ Excelå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"):
                try:
                    excel_buffer = export_to_excel_with_sheets(
                        classified_df,
                        groups
                    )
                    
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    filename = f"asin_matching_results_{timestamp}.xlsx"
                    
                    st.download_button(
                        label="ğŸ“¥ çµæœã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=excel_buffer.getvalue(),
                        file_name=filename,
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                    )
                except Exception as e:
                    st.error(f"ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        
        with tab6:
            st.header("âŒ æ¤œç´¢å¤±æ•—")
            failed_data = classified_df[
                classified_df['search_status'] != 'success'
            ]
            st.error(f"âŒ {len(failed_data)}ä»¶ã®æ¤œç´¢ãŒå¤±æ•—ã—ã¾ã—ãŸ")
            
            if not failed_data.empty:
                display_columns = ['clean_title', 'japanese_name', 'llm_source', 'search_status', 'extracted_brand', 
                                 'extracted_quantity', 'cleaned_title']
                
                available_columns = [col for col in display_columns if col in failed_data.columns]
                st.dataframe(failed_data[available_columns])
                
                # å¤±æ•—ç†ç”±ã®åˆ†æ
                if 'search_status' in failed_data.columns:
                    failure_analysis = failed_data['search_status'].value_counts()
                    st.subheader("ğŸ“Š å¤±æ•—ç†ç”±åˆ†æ")
                    st.bar_chart(failure_analysis)
    
    except Exception as e:
        st.error(f"âŒ ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
        st.write("ãƒ‡ãƒãƒƒã‚°æƒ…å ±:")
        st.write(f"å‡¦ç†ãƒ‡ãƒ¼ã‚¿å‹: {type(st.session_state.processed_data)}")
        st.write(f"ãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {st.session_state.processed_data.shape}")
        st.write(f"ã‚«ãƒ©ãƒ : {st.session_state.processed_data.columns.tolist()}")

# ä½¿ç”¨æ–¹æ³•
with st.expander("ğŸ“– ä½¿ç”¨æ–¹æ³•", expanded=False):
    st.markdown("""
    ### ğŸš€ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ—¥æœ¬èªåŒ–çµ±åˆç‰ˆã®ä½¿ç”¨æ‰‹é †
    
    #### 1. ç’°å¢ƒè¨­å®šç¢ºèª
    - SP-APIèªè¨¼æƒ…å ±ï¼ˆ.envãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
    - **OpenAI API Key**ï¼ˆGPT-4oãƒ¡ã‚¤ãƒ³ç”¨ï¼‰
    - **Gemini API Key**ï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ»æœ€æ–°å•†å“ç”¨ï¼‰
    - brands.jsonï¼ˆ249ãƒ–ãƒ©ãƒ³ãƒ‰è¾æ›¸ï¼‰
    
    #### 2. ãƒ‡ãƒ¼ã‚¿æº–å‚™
    - å•†å“åãŒå«ã¾ã‚Œã‚‹Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’æº–å‚™
    - ã€Œãƒ‡ãƒ¼ã‚¿ç®¡ç†ã€ã‚¿ãƒ–ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    - å•†å“åã‚«ãƒ©ãƒ ã‚’é¸æŠ
    
    #### 3. ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰çµ±åˆæ©Ÿèƒ½ã®æ´»ç”¨
    - **GPT-4oæ—¥æœ¬èªåŒ–**: ä¸€èˆ¬å•†å“ã®é«˜å“è³ªç¿»è¨³ï¼ˆãƒ¡ã‚¤ãƒ³ï¼‰
    - **Geminiãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—**: æœ€æ–°å•†å“ãƒ»ç‰¹æ®Šå•†å“å¯¾å¿œ
    - **é«˜å“è³ªã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°**: çµµæ–‡å­—ãƒ»å®£ä¼æ–‡å¥ã‚’è‡ªå‹•é™¤å»
    - **249ãƒ–ãƒ©ãƒ³ãƒ‰è¾æ›¸**: æ­£ç¢ºãªãƒ–ãƒ©ãƒ³ãƒ‰è­˜åˆ¥
    - **æ”¹è‰¯ã•ã‚ŒãŸä¸€è‡´åº¦**: æœ€å¤§100ç‚¹ã®è©³ç´°è©•ä¾¡
    
    #### 4. ASINæ¤œç´¢å®Ÿè¡Œãƒ•ãƒ­ãƒ¼
    1. **è‹±èªå•†å“å**: "FANCL Mild Cleansing Oil"
    2. **ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°**: ãƒã‚¤ã‚ºé™¤å»ãƒ»ãƒ–ãƒ©ãƒ³ãƒ‰æŠ½å‡º
    3. **ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ—¥æœ¬èªåŒ–**: 
       - GPT-4o: "ãƒ•ã‚¡ãƒ³ã‚±ãƒ« ãƒã‚¤ãƒ«ãƒ‰ ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚° ã‚ªã‚¤ãƒ«"
       - å¤±æ•—æ™‚Gemini: æœ€æ–°å•†å“ã‚‚å¯¾å¿œ
    4. **SP-APIæ¤œç´¢**: æ—¥æœ¬èªã§é«˜ç²¾åº¦æ¤œç´¢
    5. **çµæœè©•ä¾¡**: æ”¹è‰¯ã•ã‚ŒãŸä¸€è‡´åº¦è¨ˆç®—
    
    #### 5. çµæœç¢ºèª
    - **ã‚°ãƒ«ãƒ¼ãƒ—A**: ç¢ºå®Ÿã«ä½¿ãˆã‚‹ASINï¼ˆä¸€è‡´åº¦70%ä»¥ä¸Š & Primeå¯¾å¿œï¼‰
    - **ã‚°ãƒ«ãƒ¼ãƒ—B**: è¦ç¢ºèªASINï¼ˆä¸€è‡´åº¦50%ä»¥ä¸Š ã¾ãŸã¯ Primeå¯¾å¿œï¼‰
    - **ã‚°ãƒ«ãƒ¼ãƒ—C**: å‚è€ƒASINï¼ˆãã®ä»–ï¼‰
    
    #### 6. çµæœæ´»ç”¨
    - ASINãƒªã‚¹ãƒˆã‚’ã‚³ãƒ”ãƒ¼ã—ã¦Amazonåºƒå‘Šã§æ´»ç”¨
    - Excelå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    - æ—¥æœ¬èªåŒ–çµæœãƒ»ä¸€è‡´åº¦è©³ç´°ã§å“è³ªç¢ºèª
    
    ### ğŸ¯ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰çµ±åˆç‰ˆã®åŠ¹æœ
    - **ä¸€è‡´åº¦å‘ä¸Š**: å¾“æ¥37% â†’ **70-90%ä»¥ä¸Š**
    - **æ—¥æœ¬èªåŒ–ç²¾åº¦**: GPT-4o + Geminiãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰
    - **æœ€æ–°å•†å“å¯¾å¿œ**: Geminiã«ã‚ˆã‚‹å¹…åºƒã„ã‚«ãƒãƒ¬ãƒƒã‚¸
    - **ãƒ–ãƒ©ãƒ³ãƒ‰è­˜åˆ¥**: 249ãƒ–ãƒ©ãƒ³ãƒ‰å®Œå…¨å¯¾å¿œ
    - **æ¤œç´¢ç²¾åº¦**: æ—¥æœ¬èªæ¤œç´¢ã«ã‚ˆã‚‹é«˜ç²¾åº¦ãƒãƒƒãƒãƒ³ã‚°
    - **å®Ÿç¸¾**: æ—¢å­˜ã‚·ã‚¹ãƒ†ãƒ ã§500å“85%ä»¥ä¸Šã®æˆåŠŸç‡
    """)

# ãƒ•ãƒƒã‚¿ãƒ¼
st.markdown("---")
st.markdown("**ASINãƒãƒƒãƒãƒ³ã‚°ãƒ„ãƒ¼ãƒ« - ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ—¥æœ¬èªåŒ–çµ±åˆç‰ˆ** | Powered by OpenAI GPT-4o + Google Gemini + Amazon SP-API & Streamlit")