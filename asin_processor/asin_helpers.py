# asin_helpers.py - æ”¹è‰¯ç‰ˆã‚°ãƒ«ãƒ¼ãƒ—åˆ†é¡å¯¾å¿œï¼ˆå®Œå…¨ä¿®æ­£ç‰ˆï¼‰
import pandas as pd
import numpy as np
from datetime import datetime
import re

def classify_confidence_groups(df, high_threshold=70, medium_threshold=40):
    """
    ASINã‚’3ã¤ã®ä¿¡é ¼åº¦ã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†é¡ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
    Primeåˆ¤å®šã«ä¾å­˜ã›ãšã€ä¸€è‡´åº¦ã¨ASINå­˜åœ¨ã§åˆ†é¡
    
    A: ä¸€è‡´åº¦70%ä»¥ä¸Š & ASINæœ‰ã‚Šï¼ˆç¢ºå®Ÿã«ä½¿ãˆã‚‹ï¼‰
    B: ä¸€è‡´åº¦40-69% & ASINæœ‰ã‚Šï¼ˆè¦ç¢ºèªï¼‰
    C: ä¸€è‡´åº¦39%ä»¥ä¸‹ ã¾ãŸã¯ ASINç„¡ã—ï¼ˆå‚è€ƒï¼‰
    
    Args:
        df: å‡¦ç†ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        high_threshold: ã‚°ãƒ«ãƒ¼ãƒ—Aåˆ†é¡ã®é–¾å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ70%ï¼‰
        medium_threshold: ã‚°ãƒ«ãƒ¼ãƒ—Båˆ†é¡ã®é–¾å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ40%ï¼‰
    
    Returns:
        åˆ†é¡æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    """
    df = df.copy()
    
    # å¿…è¦ãªã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯è¿½åŠ ãƒ»è£œå®Œ
    if 'is_prime' not in df.columns:
        df['is_prime'] = False
    
    if 'relevance_score' not in df.columns:
        df['relevance_score'] = 0
    
    # ASINé–¢é€£ã‚«ãƒ©ãƒ ã®çµ±ä¸€ï¼ˆè¤‡æ•°ã®ã‚«ãƒ©ãƒ åã«å¯¾å¿œï¼‰
    asin_column = None
    for col in ['asin', 'amazon_asin', 'ASIN']:
        if col in df.columns:
            asin_column = col
            break
    
    if asin_column is None:
        df['asin'] = ''
        asin_column = 'asin'
    
    # ASINã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
    df['has_asin'] = df[asin_column].notna() & (df[asin_column] != '') & (df[asin_column] != 'N/A')
    
    # æ”¹è‰¯ç‰ˆã‚°ãƒ«ãƒ¼ãƒ—åˆ†é¡ï¼ˆPrimeåˆ¤å®šã«ä¾å­˜ã—ãªã„ï¼‰
    def classify_group(row):
        relevance = row.get('relevance_score', 0)
        has_asin = row.get('has_asin', False)
        
        # ASINç„¡ã—ã¯å¿…ãšã‚°ãƒ«ãƒ¼ãƒ—C
        if not has_asin:
            return 'C'
        
        # ASINæœ‰ã‚Šã®å ´åˆã¯ä¸€è‡´åº¦ã§åˆ†é¡
        if relevance >= high_threshold:  # 70%ä»¥ä¸Š
            return 'A'
        elif relevance >= medium_threshold:  # 40-69%
            return 'B'
        else:  # 39%ä»¥ä¸‹
            return 'C'
    
    df['confidence_group'] = df.apply(classify_group, axis=1)
    
    # Amazonå•†å“ãƒšãƒ¼ã‚¸ãƒªãƒ³ã‚¯ç”Ÿæˆ
    df['amazon_link'] = df[asin_column].apply(
        lambda asin: f"https://www.amazon.co.jp/dp/{asin}" if asin and pd.notna(asin) and asin != '' else ""
    )
    
    # åˆæœŸæ‰¿èªã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
    if 'approval_status' not in df.columns:
        # ã‚°ãƒ«ãƒ¼ãƒ—Aã¯è‡ªå‹•æ‰¿èªã€ãã®ä»–ã¯pending
        df['approval_status'] = df['confidence_group'].apply(
            lambda group: 'approved' if group == 'A' else 'pending'
        )
    
    # ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã«ã‚½ãƒ¼ãƒˆï¼ˆã‚°ãƒ«ãƒ¼ãƒ—Aâ†’Bâ†’Cã€å„ã‚°ãƒ«ãƒ¼ãƒ—å†…ã¯ä¸€è‡´åº¦é™é †ï¼‰
    df = df.sort_values(
        by=['confidence_group', 'relevance_score'], 
        ascending=[True, False]
    ).reset_index(drop=True)
    
    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±å‡ºåŠ›
    group_counts = df['confidence_group'].value_counts().sort_index()
    print(f"ğŸ“Š æ”¹è‰¯ç‰ˆã‚°ãƒ«ãƒ¼ãƒ—åˆ†é¡çµæœ:")
    print(f"   ã‚°ãƒ«ãƒ¼ãƒ—Aï¼ˆç¢ºå®Ÿï¼‰: {group_counts.get('A', 0)}ä»¶")
    print(f"   ã‚°ãƒ«ãƒ¼ãƒ—Bï¼ˆè¦ç¢ºèªï¼‰: {group_counts.get('B', 0)}ä»¶")
    print(f"   ã‚°ãƒ«ãƒ¼ãƒ—Cï¼ˆå‚è€ƒï¼‰: {group_counts.get('C', 0)}ä»¶")
    
    # ä¸€è‡´åº¦åˆ†å¸ƒã®è¡¨ç¤º
    if len(df) > 0:
        avg_relevance = df['relevance_score'].mean()
        max_relevance = df['relevance_score'].max()
        print(f"   å¹³å‡ä¸€è‡´åº¦: {avg_relevance:.1f}%")
        print(f"   æœ€é«˜ä¸€è‡´åº¦: {max_relevance:.1f}%")
    
    return df

def calculate_batch_status(df):
    """
    ãƒãƒƒãƒå‡¦ç†ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã®è¨ˆç®—ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
    """
    total_items = len(df)
    if total_items == 0:
        return {
            'total': 0,
            'processed': 0,
            'success': 0,
            'failed': 0,
            'success_rate': 0,
            'group_a': 0,
            'group_b': 0,
            'group_c': 0,
            'approved': 0,
            'rejected': 0,
            'pending': 0,
            'progress': 0
        }
    
    # ASINé–¢é€£ã‚«ãƒ©ãƒ ã®çµ±ä¸€
    asin_column = None
    for col in ['asin', 'amazon_asin', 'ASIN']:
        if col in df.columns:
            asin_column = col
            break
    
    # å‡¦ç†æ¸ˆã¿ã‚«ã‚¦ãƒ³ãƒˆï¼ˆsearch_statusãŒã‚ã‚‹å ´åˆï¼‰
    if 'search_status' in df.columns:
        processed_count = len(df[df['search_status'].notna()])
        success_count = len(df[df['search_status'] == 'success'])
        failed_count = processed_count - success_count
    elif asin_column:
        # search_statusãŒãªã„å ´åˆã¯ASINã®æœ‰ç„¡ã§åˆ¤å®š
        success_count = len(df[df[asin_column].notna() & (df[asin_column] != '') & (df[asin_column] != 'N/A')])
        processed_count = total_items  # å…¨ä»¶å‡¦ç†æ¸ˆã¿ã¨ä»®å®š
        failed_count = total_items - success_count
    else:
        processed_count = total_items
        success_count = 0
        failed_count = 0
    
    # æˆåŠŸç‡è¨ˆç®—
    success_rate = (success_count / total_items * 100) if total_items > 0 else 0
    
    # ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã®ä»¶æ•°ï¼ˆconfidence_groupãŒã‚ã‚‹å ´åˆï¼‰
    if 'confidence_group' in df.columns:
        group_a_count = len(df[df['confidence_group'] == 'A'])
        group_b_count = len(df[df['confidence_group'] == 'B'])
        group_c_count = len(df[df['confidence_group'] == 'C'])
    else:
        group_a_count = 0
        group_b_count = 0
        group_c_count = 0
    
    # æ‰¿èªã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã”ã¨ã®ä»¶æ•°
    if 'approval_status' in df.columns:
        approved_count = len(df[df['approval_status'] == 'approved'])
        rejected_count = len(df[df['approval_status'] == 'rejected'])
        pending_count = len(df[df['approval_status'] == 'pending'])
    else:
        approved_count = group_a_count
        rejected_count = 0
        pending_count = group_b_count + group_c_count
    
    # é€²æ—ç‡è¨ˆç®—
    progress_percentage = (processed_count / total_items * 100) if total_items > 0 else 0
    
    return {
        'total': total_items,
        'processed': processed_count,
        'success': success_count,
        'failed': failed_count,
        'success_rate': success_rate,
        'group_a': group_a_count,
        'group_b': group_b_count,
        'group_c': group_c_count,
        'approved': approved_count,
        'rejected': rejected_count,
        'pending': pending_count,
        'progress': progress_percentage
    }

def export_to_excel_with_sheets(df, groups=None):
    """
    ã‚°ãƒ«ãƒ¼ãƒ—A/B/Cã‚’åˆ¥ã‚·ãƒ¼ãƒˆã«æŒ¯ã‚Šåˆ†ã‘ã¦Excelãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
    """
    import io
    
    # ãƒãƒƒãƒ•ã‚¡ã‚’ä½œæˆ
    excel_buffer = io.BytesIO()
    
    # ã‚°ãƒ«ãƒ¼ãƒ—ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯è‡ªå‹•åˆ†é¡
    if groups is None:
        df_classified = classify_confidence_groups(df)
        groups = {
            'group_a': df_classified[df_classified['confidence_group'] == 'A'],
            'group_b': df_classified[df_classified['confidence_group'] == 'B'],
            'group_c': df_classified[df_classified['confidence_group'] == 'C']
        }
    
    # Excelãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    with pd.ExcelWriter(excel_buffer, engine='xlsxwriter') as writer:
        # ã‚µãƒãƒªãƒ¼ã‚·ãƒ¼ãƒˆ
        summary_data = pd.DataFrame({
            'ã‚°ãƒ«ãƒ¼ãƒ—': ['A: ç¢ºå®Ÿã«ä½¿ãˆã‚‹ASIN', 'B: è¦ç¢ºèªASIN', 'C: å‚è€ƒæƒ…å ±', 'åˆè¨ˆ'],
            'ä»¶æ•°': [
                len(groups['group_a']), 
                len(groups['group_b']), 
                len(groups['group_c']), 
                len(df)
            ],
            'å‰²åˆ': [
                f"{len(groups['group_a'])/len(df)*100:.1f}%" if len(df) > 0 else "0%", 
                f"{len(groups['group_b'])/len(df)*100:.1f}%" if len(df) > 0 else "0%", 
                f"{len(groups['group_c'])/len(df)*100:.1f}%" if len(df) > 0 else "0%",
                "100%"
            ],
            'èª¬æ˜': [
                'ä¸€è‡´åº¦70%ä»¥ä¸Šã€å³åº§ã«ä½¿ç”¨å¯èƒ½',
                'ä¸€è‡´åº¦40-69%ã€æ‰‹å‹•ç¢ºèªæ¨å¥¨',
                'ä¸€è‡´åº¦39%ä»¥ä¸‹ã¾ãŸã¯æ¤œç´¢å¤±æ•—',
                'å…¨ä½“ãƒ‡ãƒ¼ã‚¿'
            ]
        })
        summary_data.to_excel(writer, sheet_name='ğŸ“Š_ã‚µãƒãƒªãƒ¼', index=False)
        
        # å„ã‚°ãƒ«ãƒ¼ãƒ—ã®ã‚·ãƒ¼ãƒˆ
        if len(groups['group_a']) > 0:
            groups['group_a'].to_excel(writer, sheet_name='âœ…_ã‚°ãƒ«ãƒ¼ãƒ—A_ç¢ºå®Ÿ', index=False)
        
        if len(groups['group_b']) > 0:
            groups['group_b'].to_excel(writer, sheet_name='âš ï¸_ã‚°ãƒ«ãƒ¼ãƒ—B_è¦ç¢ºèª', index=False)
        
        if len(groups['group_c']) > 0:
            groups['group_c'].to_excel(writer, sheet_name='ğŸ“_ã‚°ãƒ«ãƒ¼ãƒ—C_å‚è€ƒ', index=False)
        
        # å…¨ãƒ‡ãƒ¼ã‚¿ã‚·ãƒ¼ãƒˆ
        df.to_excel(writer, sheet_name='ğŸ“‹_å…¨ãƒ‡ãƒ¼ã‚¿', index=False)
        
        # ãƒ¯ãƒ¼ã‚¯ãƒ–ãƒƒã‚¯å–å¾—ã—ã¦ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆè¨­å®š
        workbook = writer.book
        
        # ã‚·ãƒ¼ãƒˆæ›¸å¼è¨­å®š
        for sheet_name in writer.sheets:
            worksheet = writer.sheets[sheet_name]
            
            # åˆ—å¹…ã®èª¿æ•´
            if 'ã‚µãƒãƒªãƒ¼' not in sheet_name:
                worksheet.set_column('A:A', 12)  # ASINåˆ—
                worksheet.set_column('B:D', 35)  # å•†å“åãƒ»æ—¥æœ¬èªåŒ–åˆ—
                worksheet.set_column('E:H', 15)  # ãã®ä»–ã®åˆ—
                worksheet.set_column('I:Z', 12)  # æ®‹ã‚Šã®åˆ—
            
            # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œæ›¸å¼è¨­å®š
            header_format = workbook.add_format({
                'bold': True,
                'text_wrap': True,
                'valign': 'top',
                'bg_color': '#E6F3FF',
                'border': 1,
                'font_size': 10
            })
            
            # ãƒ‡ãƒ¼ã‚¿è¡Œæ›¸å¼è¨­å®š
            cell_format = workbook.add_format({
                'text_wrap': True,
                'valign': 'top',
                'border': 1,
                'font_size': 9
            })
    
    excel_buffer.seek(0)
    return excel_buffer

def update_approval_status(df, item_id, status):
    """
    æ‰¿èªçŠ¶æ…‹ã‚’æ›´æ–°
    
    Args:
        df: ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        item_id: ã‚¢ã‚¤ãƒ†ãƒ IDï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼‰
        status: æ–°ã—ã„çŠ¶æ…‹ï¼ˆ'approved', 'rejected', 'pending'ï¼‰
    
    Returns:
        æ›´æ–°ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    """
    if item_id in df.index:
        df.at[item_id, 'approval_status'] = status
        
        # æ‰¿èªæ™‚ã¯è‡ªå‹•çš„ã«ã‚°ãƒ«ãƒ¼ãƒ—Aã«æ˜‡æ ¼
        if status == 'approved':
            df.at[item_id, 'confidence_group'] = 'A'
        # å´ä¸‹æ™‚ã¯ã‚°ãƒ«ãƒ¼ãƒ—Cã«é™æ ¼
        elif status == 'rejected':
            df.at[item_id, 'confidence_group'] = 'C'
    
    return df

def promote_to_group_a(df, item_ids):
    """
    æŒ‡å®šã•ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ ã‚’ã‚°ãƒ«ãƒ¼ãƒ—Aã«æ˜‡æ ¼
    
    Args:
        df: ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        item_ids: æ˜‡æ ¼ã•ã›ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã®IDãƒªã‚¹ãƒˆ
    
    Returns:
        æ›´æ–°ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    """
    for item_id in item_ids:
        if item_id in df.index:
            df.at[item_id, 'confidence_group'] = 'A'
            df.at[item_id, 'approval_status'] = 'approved'
    
    return df

def generate_demo_data(n_rows=10):
    """
    ãƒ‡ãƒ¢ç”¨ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆï¼ˆæ”¹è‰¯ç‰ˆï¼‰
    """
    np.random.seed(42)  # å†ç¾æ€§ã®ãŸã‚
    
    # ã‚ˆã‚Šç¾å®Ÿçš„ãªã‚µãƒ³ãƒ—ãƒ«å•†å“ãƒ‡ãƒ¼ã‚¿ï¼ˆä¸€è‡´åº¦ã‚’ç¾å®Ÿçš„ãªåˆ†å¸ƒã«ï¼‰
    products = [
        # é«˜ä¸€è‡´åº¦å•†å“ï¼ˆã‚°ãƒ«ãƒ¼ãƒ—Aæƒ³å®šï¼‰
        ('FANCL mild cleansing oil 120ml', 'ãƒ•ã‚¡ãƒ³ã‚±ãƒ« ãƒã‚¤ãƒ«ãƒ‰ ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚° ã‚ªã‚¤ãƒ« 120ml', 85),
        ('MILBON elujuda hair treatment', 'ãƒŸãƒ«ãƒœãƒ³ ã‚¨ãƒ«ã‚¸ãƒ¥ãƒ¼ãƒ€ ãƒ˜ã‚¢ãƒˆãƒªãƒ¼ãƒˆãƒ¡ãƒ³ãƒˆ', 78),
        ('LEBEL iau serum shampoo 1000ml', 'ãƒ«ãƒ™ãƒ« ã‚¤ã‚ª ã‚»ãƒ©ãƒ  ã‚·ãƒ£ãƒ³ãƒ—ãƒ¼ 1000ml', 82),
        
        # ä¸­ä¸€è‡´åº¦å•†å“ï¼ˆã‚°ãƒ«ãƒ¼ãƒ—Bæƒ³å®šï¼‰  
        ('YOLU calm night repair', 'ãƒ¨ãƒ« ã‚«ãƒ¼ãƒ  ãƒŠã‚¤ãƒˆ ãƒªãƒšã‚¢', 65),
        ('ORBIS essence moisturizer', 'ã‚ªãƒ«ãƒ“ã‚¹ ã‚¨ãƒƒã‚»ãƒ³ã‚¹ ãƒ¢ã‚¤ã‚¹ãƒãƒ£ãƒ©ã‚¤ã‚¶ãƒ¼', 58),
        ('TSUBAKI premium moist shampoo', 'ãƒ„ãƒã‚­ ãƒ—ãƒ¬ãƒŸã‚¢ãƒ  ãƒ¢ã‚¤ã‚¹ãƒˆ ã‚·ãƒ£ãƒ³ãƒ—ãƒ¼', 52),
        
        # ä½ä¸€è‡´åº¦å•†å“ï¼ˆã‚°ãƒ«ãƒ¼ãƒ—Cæƒ³å®šï¼‰
        ('generic hair oil treatment', 'ãƒ˜ã‚¢ã‚ªã‚¤ãƒ« ãƒˆãƒªãƒ¼ãƒˆãƒ¡ãƒ³ãƒˆ', 32),
        ('unknown brand face cream', 'ãƒ•ã‚§ã‚¤ã‚¹ã‚¯ãƒªãƒ¼ãƒ ', 28),
        ('basic cleansing foam', 'ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚° ãƒ•ã‚©ãƒ¼ãƒ ', 35),
        ('simple moisturizing lotion', 'ãƒ¢ã‚¤ã‚¹ãƒãƒ£ãƒ©ã‚¤ã‚¸ãƒ³ã‚° ãƒ­ãƒ¼ã‚·ãƒ§ãƒ³', 25),
    ]
    
    # å¿…è¦ãªã‚‰è¡Œæ•°ã‚’èª¿æ•´
    if n_rows > len(products):
        products = products * (n_rows // len(products) + 1)
    products = products[:n_rows]
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ç”Ÿæˆ
    data_rows = []
    for i, (eng_name, jp_name, relevance) in enumerate(products):
        # ä¸€è‡´åº¦ã«åŸºã¥ã„ã¦ASINã®æœ‰ç„¡ã‚’æ±ºå®šï¼ˆä½ã„å ´åˆã¯æ¤œç´¢å¤±æ•—ã™ã‚‹å¯èƒ½æ€§ï¼‰
        has_asin = relevance > 30 and np.random.random() > 0.1  # 30%ä»¥ä¸‹ã¯é«˜ç¢ºç‡ã§æ¤œç´¢å¤±æ•—
        
        data_rows.append({
            'clean_title': eng_name,
            'japanese_name': jp_name,
            'llm_source': 'GPT-4o' if np.random.random() > 0.2 else 'Gemini',  # 80%ã¯GPT-4o
            'amazon_asin': f"B{i+1:09d}" if has_asin else '',
            'amazon_title': jp_name if has_asin else '',
            'amazon_brand': jp_name.split()[0] if has_asin and ' ' in jp_name else '',
            'relevance_score': relevance + np.random.randint(-5, 6),  # Â±5ã®ãƒ©ãƒ³ãƒ€ãƒ èª¿æ•´
            'is_prime': np.random.choice([True, False], p=[0.7, 0.3]),
            'search_status': 'success' if has_asin else 'no_results',
            'price': f"Â¥{np.random.randint(800, 8000)}" if has_asin else '',
            'extracted_brand': jp_name.split()[0] if ' ' in jp_name else '',
            'extracted_quantity': f"{np.random.choice(['120ml', '200ml', '500ml', '1000ml'])}" if np.random.random() > 0.3 else '',
            'relevance_details': f"ãƒ–ãƒ©ãƒ³ãƒ‰ä¸€è‡´: +25ç‚¹, é‡è¦èªä¸€è‡´: +{np.random.randint(10,20)}ç‚¹"
        })
    
    df = pd.DataFrame(data_rows)
    
    # relevance_scoreã‚’0-100ã®ç¯„å›²ã«åˆ¶é™
    df['relevance_score'] = df['relevance_score'].clip(0, 100)
    
    return df

def analyze_classification_quality(df):
    """
    åˆ†é¡å“è³ªã®åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    
    Args:
        df: åˆ†é¡æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    
    Returns:
        dict: åˆ†æçµæœ
    """
    if 'confidence_group' not in df.columns:
        return {"error": "åˆ†é¡ãŒå®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã›ã‚“"}
    
    total = len(df)
    if total == 0:
        return {"error": "ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™"}
    
    group_counts = df['confidence_group'].value_counts()
    
    # ä¸€è‡´åº¦çµ±è¨ˆ
    relevance_stats = df.groupby('confidence_group')['relevance_score'].agg(['count', 'mean', 'min', 'max'])
    
    # ASINå–å¾—ç‡
    asin_column = None
    for col in ['asin', 'amazon_asin', 'ASIN']:
        if col in df.columns:
            asin_column = col
            break
    
    asin_rates = {}
    if asin_column:
        for group in ['A', 'B', 'C']:
            group_df = df[df['confidence_group'] == group]
            if len(group_df) > 0:
                asin_success = len(group_df[group_df[asin_column].notna() & (group_df[asin_column] != '')])
                asin_rates[group] = (asin_success / len(group_df)) * 100
            else:
                asin_rates[group] = 0
    
    return {
        "total_items": total,
        "group_distribution": {
            "A": group_counts.get('A', 0),
            "B": group_counts.get('B', 0), 
            "C": group_counts.get('C', 0)
        },
        "group_percentages": {
            "A": (group_counts.get('A', 0) / total) * 100,
            "B": (group_counts.get('B', 0) / total) * 100,
            "C": (group_counts.get('C', 0) / total) * 100
        },
        "relevance_stats": relevance_stats.to_dict() if len(relevance_stats) > 0 else {},
        "asin_success_rates": asin_rates,
        "quality_score": calculate_quality_score(df)
    }

def calculate_quality_score(df):
    """
    åˆ†é¡å“è³ªã‚¹ã‚³ã‚¢ã®è¨ˆç®—
    """
    if len(df) == 0:
        return 0
    
    group_a_count = len(df[df['confidence_group'] == 'A'])
    high_relevance_count = len(df[df['relevance_score'] >= 70])
    
    # ã‚°ãƒ«ãƒ¼ãƒ—Aã®ç²¾åº¦ï¼ˆé«˜ä¸€è‡´åº¦å•†å“ãŒã¡ã‚ƒã‚“ã¨ã‚°ãƒ«ãƒ¼ãƒ—Aã«åˆ†é¡ã•ã‚Œã¦ã„ã‚‹ã‹ï¼‰
    precision_a = (group_a_count / high_relevance_count) if high_relevance_count > 0 else 0
    
    # å…¨ä½“çš„ãªãƒãƒ©ãƒ³ã‚¹ï¼ˆæ¥µç«¯ã«åã£ã¦ã„ãªã„ã‹ï¼‰
    group_balance = 1 - abs(0.5 - (group_a_count / len(df)))  # ç†æƒ³ã¯å…¨ä½“ã®30-50%ãŒã‚°ãƒ«ãƒ¼ãƒ—A
    
    # ç·åˆå“è³ªã‚¹ã‚³ã‚¢
    quality_score = (precision_a * 0.7 + group_balance * 0.3) * 100
    
    return min(quality_score, 100)

# ãƒ†ã‚¹ãƒˆé–¢æ•°
def test_improved_classification():
    """
    æ”¹è‰¯ç‰ˆåˆ†é¡ã®ãƒ†ã‚¹ãƒˆ
    """
    print("ğŸ§ª æ”¹è‰¯ç‰ˆã‚°ãƒ«ãƒ¼ãƒ—åˆ†é¡ãƒ†ã‚¹ãƒˆé–‹å§‹")
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    test_df = generate_demo_data(20)
    
    print(f"\nğŸ“Š ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(test_df)}ä»¶")
    print("ä¸€è‡´åº¦åˆ†å¸ƒ:")
    for _, row in test_df.iterrows():
        print(f"  {row['clean_title'][:30]}... â†’ {row['relevance_score']}%")
    
    # åˆ†é¡å®Ÿè¡Œ
    classified_df = classify_confidence_groups(test_df)
    
    # åˆ†æå®Ÿè¡Œ
    analysis = analyze_classification_quality(classified_df)
    
    print(f"\nğŸ“ˆ åˆ†é¡çµæœåˆ†æ:")
    print(f"  ã‚°ãƒ«ãƒ¼ãƒ—A: {analysis['group_distribution']['A']}ä»¶ ({analysis['group_percentages']['A']:.1f}%)")
    print(f"  ã‚°ãƒ«ãƒ¼ãƒ—B: {analysis['group_distribution']['B']}ä»¶ ({analysis['group_percentages']['B']:.1f}%)")
    print(f"  ã‚°ãƒ«ãƒ¼ãƒ—C: {analysis['group_distribution']['C']}ä»¶ ({analysis['group_percentages']['C']:.1f}%)")
    print(f"  å“è³ªã‚¹ã‚³ã‚¢: {analysis['quality_score']:.1f}/100")
    
    return classified_df, analysis

if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    test_df, analysis = test_improved_classification()
    print("\nâœ… æ”¹è‰¯ç‰ˆã‚°ãƒ«ãƒ¼ãƒ—åˆ†é¡ãƒ†ã‚¹ãƒˆå®Œäº†")