def advanced_product_name_cleansing(text):
    """é«˜å“è³ªå•†å“åã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ï¼ˆè‹±æ—¥ç¿»è¨³æ©Ÿèƒ½çµ±åˆï¼‰"""
    if not text:
        return ""
    
    # Unicodeæ­£è¦åŒ–ï¼ˆNFKCï¼‰- æ—¢å­˜cleansing.pyæ©Ÿèƒ½
    text = unicodedata.normalize('NFKC', text)
    
    # é™¤å»å¯¾è±¡ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ—¢å­˜ã®é«˜å“è³ªæ©Ÿèƒ½ã‚’çµ±åˆï¼‰
    remove_patterns = [
        # çµµæ–‡å­—ãƒ»è¨˜å·
        r'[ğŸ…¹ğŸ…¿ğŸ‡¯ğŸ‡µâ˜…â˜†â€»â—â—‹â—â–²â–³â–¼â–½â– â–¡â—†â—‡â™¦â™¢â™ â™£â™¥â™¡]',
        r'[\u2600-\u26FF\u2700-\u27BF]',  # ãã®ä»–è¨˜å·
        
        # åœ¨åº«ãƒ»é…é€æƒ…å ±
        r'\[.*?stock.*?\]',
        r'\[.*?åœ¨åº«.*?\]',
        r'é€æ–™ç„¡æ–™',
        r'é…é€ç„¡æ–™',
        r'Free shipping',
        
        # å®£ä¼æ–‡å¥ãƒ»å“è³ªè¡¨ç¤º
        r'100% Authentic',
        r'made in japan',
        r'original',
        r'Direct from japan',
        r'Guaranteed authentic',
        r'æ­£è¦å“',
        r'æœ¬ç‰©',
        r'æ–°å“',
        r'æœªä½¿ç”¨',
        
        # è²©å£²è€…æƒ…å ±
        r'@.*',
        r'by.*store',
        r'shop.*',
        
        # å†—é•·ãªèª¬æ˜
        r'hair care liquid',
        r'beauty product',
        r'cosmetic',
    ]
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³é™¤å»
    for pattern in remove_patterns:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE)
    
    # åœ°åŸŸæƒ…å ±ã®é™¤å»ï¼ˆå…ˆé ­ã®ã¿ï¼‰
    text = re.sub(r'^(Japan|Global|Korean|China)\s+', '', text, flags=re.IGNORECASE)
    
    # è¤‡æ•°å•†å“ã®åˆ†é›¢ï¼ˆæœ€åˆ# sp_api_service.py - æ—¢å­˜æ©Ÿèƒ½å®Œå…¨çµ±åˆç‰ˆï¼ˆGPT-4oæ—¥æœ¬èªåŒ–çµ±åˆï¼‰
from sp_api.api import CatalogItems
from sp_api.base import Marketplaces, SellingApiException
import time
import os
from dotenv import load_dotenv
import re
import pandas as pd
import json
import unicodedata
from pathlib import Path
import streamlit as st
import openai
import google.generativeai as genai

# .envèª­ã¿è¾¼ã¿ï¼ˆshopeeç›´ä¸‹ã®.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ï¼‰
current_dir = Path(__file__).parent
parent_dir = current_dir.parent
env_path = parent_dir / '.env'
load_dotenv(env_path)

def get_japanese_name_from_gpt4o(clean_title):
    """GPT-4oã«ã‚ˆã‚‹é«˜å“è³ªæ—¥æœ¬èªåŒ–ï¼ˆæ—¢å­˜llm_service.pyçµ±åˆï¼‰"""
    try:
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            return None, "OpenAI API Key not found"
            
        client = openai.OpenAI(api_key=api_key)
        # å˜èªåˆ†å‰²ã—ã‚„ã™ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«æ”¹å–„
        prompt = f"æ¬¡ã®è‹±èªã®å•†å“åã‚’ã€æ—¥æœ¬ã®ECã‚µã‚¤ãƒˆã§é€šã˜ã‚‹è‡ªç„¶ãªæ—¥æœ¬èªã®å•†å“åã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚å„å˜èªã¯åŠè§’ã‚¹ãƒšãƒ¼ã‚¹ã§åŒºåˆ‡ã‚Šã€ãƒ–ãƒ©ãƒ³ãƒ‰ã‚„å®¹é‡ã‚‚æ—¥æœ¬èªã§è¡¨è¨˜ã—ã€èª¬æ˜ã‚„ä½™è¨ˆãªèªå¥ã¯ä¸è¦ï¼š\n\n{clean_title}"
        
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=64,
            temperature=0.3,
        )
        
        japanese_name = response.choices[0].message.content.strip()
        print(f"   ğŸ¤– GPT-4oæ—¥æœ¬èªåŒ–: {clean_title} â†’ {japanese_name}")
        return japanese_name, "GPT-4o"
        
    except Exception as e:
        print(f"   âŒ GPT-4oæ—¥æœ¬èªåŒ–å¤±æ•—: {e}")
        return None, f"GPT-4o Error: {e}"

def get_japanese_name_from_gemini(clean_title):
    """Geminiã«ã‚ˆã‚‹æ—¥æœ¬èªåŒ–ï¼ˆæ—¢å­˜llm_service.pyçµ±åˆãƒ»ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ç”¨ï¼‰"""
    try:
        api_key = os.getenv('GEMINI_API_KEY')
        if not api_key:
            return None, "Gemini API Key not found"
            
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-1.5-pro-latest')
        prompt = f"æ¬¡ã®è‹±èªã®å•†å“åã‚’ã€æ—¥æœ¬èªã®å•†å“åã«è‡ªç„¶ã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚ãƒ–ãƒ©ãƒ³ãƒ‰ã‚„å®¹é‡ã‚‚è‡ªç„¶ãªæ—¥æœ¬èªã§ã€‚ä½™è¨ˆãªèª¬æ˜ä¸è¦ï¼š\n\n{clean_title}"
        
        response = model.generate_content(prompt)
        japanese_name = response.text.strip()
        print(f"   ğŸ”® Geminiæ—¥æœ¬èªåŒ–: {clean_title} â†’ {japanese_name}")
        return japanese_name, "Gemini"
        
    except Exception as e:
        print(f"   âŒ Geminiæ—¥æœ¬èªåŒ–å¤±æ•—: {e}")
        return None, f"Gemini Error: {e}"

def get_japanese_name_hybrid(clean_title):
    """ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ—¥æœ¬èªåŒ–ï¼ˆæ—¢å­˜llm_service.pyå®Œå…¨çµ±åˆï¼‰"""
    print(f"   ğŸš€ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ—¥æœ¬èªåŒ–é–‹å§‹: {clean_title}")
    
    # ã‚¹ãƒ†ãƒƒãƒ—1: GPT-4oã‚’è©¦è¡Œï¼ˆãƒ¡ã‚¤ãƒ³ï¼‰
    jp_name, source = get_japanese_name_from_gpt4o(clean_title)
    
    if jp_name and not jp_name.isspace() and "å¤‰æ›ä¸å¯" not in jp_name:
        print(f"   âœ… GPT-4oæˆåŠŸ: {jp_name}")
        return jp_name, source
    else:
        print(f"   âš ï¸ GPT-4oå¤±æ•—ã€Geminiã§ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Ÿè¡Œ...")
    
    # ã‚¹ãƒ†ãƒƒãƒ—2: Geminiã§ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆæœ€æ–°å•†å“å¯¾å¿œï¼‰
    jp_name, source = get_japanese_name_from_gemini(clean_title)
    
    if jp_name and not jp_name.isspace() and "å¤‰æ›ä¸å¯" not in jp_name:
        print(f"   âœ… Geminiãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—æˆåŠŸ: {jp_name}")
        return jp_name, source
    else:
        print(f"   âŒ ä¸¡æ–¹å¤±æ•—ã€å…ƒã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’ä½¿ç”¨: {clean_title}")
    
    # ã‚¹ãƒ†ãƒƒãƒ—3: ä¸¡æ–¹å¤±æ•—æ™‚ã¯å…ƒã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’è¿”ã™
    return clean_title, "Original"

def get_credentials():
    """SP-APIèªè¨¼æƒ…å ±å–å¾—"""
    lwa_app_id = os.getenv("LWA_APP_ID")
    lwa_client_secret = os.getenv("LWA_CLIENT_SECRET")
    refresh_token = os.getenv("SP_API_REFRESH_TOKEN")
    
    print(f"ğŸ“ .envãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿: {env_path}")
    print(f"LWA_APP_ID: {'ã‚ã‚Š' if lwa_app_id else 'ãªã—'}")
    print(f"LWA_CLIENT_SECRET: {'ã‚ã‚Š' if lwa_client_secret else 'ãªã—'}")
    print(f"SP_API_REFRESH_TOKEN: {'ã‚ã‚Š' if refresh_token else 'ãªã—'}")
    print(f"OPENAI_API_KEY: {'ã‚ã‚Š' if os.getenv('OPENAI_API_KEY') else 'ãªã—'}")
    print(f"GEMINI_API_KEY: {'ã‚ã‚Š' if os.getenv('GEMINI_API_KEY') else 'ãªã—'}")
    
    if not all([lwa_app_id, lwa_client_secret, refresh_token]):
        print("âŒ ç’°å¢ƒå¤‰æ•°ãŒä¸è¶³ã—ã¦ã„ã¾ã™")
        return None
    
    return {
        "lwa_app_id": lwa_app_id,
        "lwa_client_secret": lwa_client_secret,
        "refresh_token": refresh_token
    }

def load_brand_dict():
    """é«˜å“è³ªãƒ–ãƒ©ãƒ³ãƒ‰è¾æ›¸ã®èª­ã¿è¾¼ã¿ï¼ˆæ—¢å­˜brands.jsonæ´»ç”¨ï¼‰"""
    try:
        # æ­£ã—ã„ãƒ‘ã‚¹: /workspaces/shopee/data/brands.json
        brands_file_path = current_dir.parent / 'data' / 'brands.json'
        
        if brands_file_path.exists():
            with open(brands_file_path, 'r', encoding='utf-8') as f:
                existing_brands = json.load(f)
            
            print(f"ğŸ“š æ—¢å­˜brands.jsonã‹ã‚‰èª­ã¿è¾¼ã¿: {len(existing_brands)}ãƒ–ãƒ©ãƒ³ãƒ‰")
            print(f"ğŸ“ èª­ã¿è¾¼ã¿ãƒ‘ã‚¹: {brands_file_path}")
            
            # ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®ãƒ‡ãƒãƒƒã‚°
            if existing_brands:
                first_key = list(existing_brands.keys())[0]
                first_value = existing_brands[first_key]
                print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚µãƒ³ãƒ—ãƒ«:")
                print(f"   ã‚­ãƒ¼ä¾‹: {first_key}")
                print(f"   å€¤ä¾‹: {first_value} (å‹: {type(first_value)})")
                
                # å€¤ã®å‹åˆ†æ
                value_types = {}
                for key, value in list(existing_brands.items())[:5]:  # æœ€åˆã®5ä»¶ã‚’åˆ†æ
                    value_type = type(value).__name__
                    if value_type not in value_types:
                        value_types[value_type] = []
                    value_types[value_type].append(f"{key}: {value}")
                
                print(f"   å€¤ã®å‹åˆ†å¸ƒ: {list(value_types.keys())}")
                for vtype, examples in value_types.items():
                    print(f"     {vtype}: {examples[0]}")
            
            return existing_brands
        else:
            print(f"âš ï¸ brands.jsonãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {brands_file_path}")
            print("ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¾æ›¸ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
    except Exception as e:
        print(f"âš ï¸ brands.jsonèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}ã€‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¾æ›¸ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
        import traceback
        print("è©³ç´°ã‚¨ãƒ©ãƒ¼:")
        traceback.print_exc()
    
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªãƒ–ãƒ©ãƒ³ãƒ‰è¾æ›¸ï¼ˆbrands.jsonãŒãªã„å ´åˆï¼‰
    fallback_brands = {
        # åŒ–ç²§å“ãƒ»ã‚¹ã‚­ãƒ³ã‚±ã‚¢
        "FANCL": ["ãƒ•ã‚¡ãƒ³ã‚±ãƒ«", "fancl"],
        "ORBIS": ["ã‚ªãƒ«ãƒ“ã‚¹", "orbis"],
        "SK-II": ["ã‚¨ã‚¹ã‚±ãƒ¼ãƒ„ãƒ¼", "SK2", "SK-2"],
        "SHISEIDO": ["è³‡ç”Ÿå ‚", "shiseido"],
        "KANEBO": ["ã‚«ãƒãƒœã‚¦", "kanebo"],
        "KOSE": ["ã‚³ãƒ¼ã‚»ãƒ¼", "kose"],
        "POLA": ["ãƒãƒ¼ãƒ©", "pola"],
        "ALBION": ["ã‚¢ãƒ«ãƒ“ã‚ªãƒ³", "albion"],
        "HABA": ["ãƒãƒ¼ãƒãƒ¼", "haba"],
        "DHC": ["ãƒ‡ã‚£ãƒ¼ã‚¨ã‚¤ãƒã‚·ãƒ¼", "dhc"],
        
        # ãƒ˜ã‚¢ã‚±ã‚¢
        "MILBON": ["ãƒŸãƒ«ãƒœãƒ³", "milbon"],
        "LEBEL": ["ãƒ«ãƒ™ãƒ«", "lebel"],
        "YOLU": ["ãƒ¨ãƒ«", "yolu"],
        "TSUBAKI": ["æ¤¿", "tsubaki"],
        "LISSAGE": ["ãƒªã‚µãƒ¼ã‚¸", "lissage"],
        "KERASTASE": ["ã‚±ãƒ©ã‚¹ã‚¿ãƒ¼ã‚¼", "kerastase"],
        
        # å®¶é›»ãƒ»ç¾å®¹æ©Ÿå™¨
        "PANASONIC": ["ãƒ‘ãƒŠã‚½ãƒ‹ãƒƒã‚¯", "panasonic"],
        "PHILIPS": ["ãƒ•ã‚£ãƒªãƒƒãƒ—ã‚¹", "philips"],
        "KOIZUMI": ["ã‚³ã‚¤ã‚ºãƒŸ", "koizumi"],
        "HITACHI": ["æ—¥ç«‹", "hitachi"],
        
        # å¥åº·é£Ÿå“ãƒ»ã‚µãƒ—ãƒª
        "SUNTORY": ["ã‚µãƒ³ãƒˆãƒªãƒ¼", "suntory"],
        "ASAHI": ["ã‚¢ã‚µãƒ’", "asahi"],
        "MEIJI": ["æ˜æ²»", "meiji"],
        "MORINAGA": ["æ£®æ°¸", "morinaga"],
        
        # è¿½åŠ ãƒ–ãƒ©ãƒ³ãƒ‰ï¼ˆbrands.jsonã‹ã‚‰ã®ä¸»è¦ãƒ–ãƒ©ãƒ³ãƒ‰ï¼‰
        "SONY": ["ã‚½ãƒ‹ãƒ¼", "sony"],
        "NINTENDO": ["ä»»å¤©å ‚", "nintendo"],
        "APPLE": ["ã‚¢ãƒƒãƒ—ãƒ«", "apple"],
        "SAMSUNG": ["ã‚µãƒ ã‚¹ãƒ³", "samsung"],
        "LG": ["ã‚¨ãƒ«ã‚¸ãƒ¼", "lg"],
        "SHARP": ["ã‚·ãƒ£ãƒ¼ãƒ—", "sharp"],
        "TOSHIBA": ["æ±èŠ", "toshiba"],
        "FUJITSU": ["å¯Œå£«é€š", "fujitsu"],
        "NEC": ["ã‚¨ãƒŒã‚¤ãƒ¼ã‚·ãƒ¼", "nec"],
    }
    
    print(f"ğŸ“š ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¾æ›¸ä½¿ç”¨: {len(fallback_brands)}ãƒ–ãƒ©ãƒ³ãƒ‰")
    return fallback_brands

def advanced_product_name_cleansing(text):
    """é«˜å“è³ªå•†å“åã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ï¼ˆæ—¢å­˜cleansing.pyæ©Ÿèƒ½çµ±åˆï¼‰"""
    if not text:
        return ""
    
    # Unicodeæ­£è¦åŒ–ï¼ˆNFKCï¼‰- æ—¢å­˜cleansing.pyæ©Ÿèƒ½
    text = unicodedata.normalize('NFKC', text)
    
    # é™¤å»å¯¾è±¡ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ—¢å­˜ã®é«˜å“è³ªæ©Ÿèƒ½ã‚’çµ±åˆï¼‰
    remove_patterns = [
        # çµµæ–‡å­—ãƒ»è¨˜å·
        r'[ğŸ…¹ğŸ…¿ğŸ‡¯ğŸ‡µâ˜…â˜†â€»â—â—‹â—â–²â–³â–¼â–½â– â–¡â—†â—‡â™¦â™¢â™ â™£â™¥â™¡]',
        r'[\u2600-\u26FF\u2700-\u27BF]',  # ãã®ä»–è¨˜å·
        
        # åœ¨åº«ãƒ»é…é€æƒ…å ±
        r'\[.*?stock.*?\]',
        r'\[.*?åœ¨åº«.*?\]',
        r'é€æ–™ç„¡æ–™',
        r'é…é€ç„¡æ–™',
        r'Free shipping',
        
        # å®£ä¼æ–‡å¥ãƒ»å“è³ªè¡¨ç¤º
        r'100% Authentic',
        r'made in japan',
        r'original',
        r'Direct from japan',
        r'Guaranteed authentic',
        r'æ­£è¦å“',
        r'æœ¬ç‰©',
        r'æ–°å“',
        r'æœªä½¿ç”¨',
        
        # è²©å£²è€…æƒ…å ±
        r'@.*',
        r'by.*store',
        r'shop.*',
        
        # å†—é•·ãªèª¬æ˜
        r'hair care liquid',
        r'beauty product',
        r'cosmetic',
    ]
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³é™¤å»
    for pattern in remove_patterns:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE)
    
    # åœ°åŸŸæƒ…å ±ã®é™¤å»ï¼ˆå…ˆé ­ã®ã¿ï¼‰
    text = re.sub(r'^(Japan|Global|Korean|China)\s+', '', text, flags=re.IGNORECASE)
    
    # è¤‡æ•°å•†å“ã®åˆ†é›¢ï¼ˆæœ€åˆã®å•†å“ã®ã¿æŠ½å‡ºï¼‰
    if '/' in text and len(text.split('/')) > 1:
        text = text.split('/')[0].strip()
    
    # ä½™åˆ†ãªç©ºç™½ãƒ»è¨˜å·ã®é™¤å»
    text = re.sub(r'\s+', ' ', text).strip()
    text = re.sub(r'^[^\w\s]*|[^\w\s]*$', '', text)
    
    return text

def extract_brand_and_quantity(text, brand_dict):
    """é«˜å“è³ªãƒ–ãƒ©ãƒ³ãƒ‰ãƒ»æ•°é‡æŠ½å‡ºï¼ˆextractors1.txtæ©Ÿèƒ½çµ±åˆï¼‰"""
    if not text:
        return {"brand": None, "quantity": None, "cleaned_text": text}
    
    # å•†å“åã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
    cleaned_text = advanced_product_name_cleansing(text)
    
    # ãƒ–ãƒ©ãƒ³ãƒ‰æŠ½å‡ºï¼ˆå„ªå…ˆé †ä½ä»˜ãï¼‰
    detected_brand = None
    max_priority = 0
    
    for brand, variations in brand_dict.items():
        # variations ãŒæ–‡å­—åˆ—ã®å ´åˆã¯ãƒªã‚¹ãƒˆã«å¤‰æ›
        if isinstance(variations, str):
            variations = [variations]
        elif not isinstance(variations, list):
            variations = []
        
        # brandã¨variationsã‚’çµ±åˆã—ã¦ãƒã‚§ãƒƒã‚¯
        all_variations = variations + [brand]
        
        for variation in all_variations:
            if not variation or not isinstance(variation, str):
                continue
                
            # å„ªå…ˆé †ä½ï¼ˆã‚«ã‚¿ã‚«ãƒŠ>æ¼¢å­—>ã²ã‚‰ãŒãª>è‹±å­—ï¼‰
            if re.search(r'[\u30A0-\u30FF]', variation):  # ã‚«ã‚¿ã‚«ãƒŠ
                priority = 4
            elif re.search(r'[\u4E00-\u9FFF]', variation):  # æ¼¢å­—
                priority = 3
            elif re.search(r'[\u3040-\u309F]', variation):  # ã²ã‚‰ãŒãª
                priority = 2
            else:  # è‹±å­—
                priority = 1
            
            # ãƒ–ãƒ©ãƒ³ãƒ‰åã®æ¤œå‡º
            try:
                if re.search(rf'\b{re.escape(variation)}\b', cleaned_text, re.IGNORECASE):
                    if priority > max_priority:
                        detected_brand = brand
                        max_priority = priority
            except Exception as e:
                print(f"âš ï¸ ãƒ–ãƒ©ãƒ³ãƒ‰æ¤œç´¢ã‚¨ãƒ©ãƒ¼ (variation: {variation}): {e}")
                continue
    
    # æ•°é‡æŠ½å‡º
    quantity_pattern = r'(\d+(?:\.\d+)?)\s*(ml|g|kg|oz|L|â„“|cc|å€‹|æœ¬|æš|éŒ |ç²’|åŒ…|è¢‹)'
    quantity_match = re.search(quantity_pattern, cleaned_text, re.IGNORECASE)
    extracted_quantity = quantity_match.group() if quantity_match else None
    
    return {
        "brand": detected_brand,
        "quantity": extracted_quantity,
        "cleaned_text": cleaned_text
    }

def calculate_enhanced_relevance_score(original_title, amazon_title, amazon_brand, extracted_info):
    """æ”¹è‰¯ã•ã‚ŒãŸä¸€è‡´åº¦è¨ˆç®—ï¼ˆæœ€å¤§100ç‚¹ï¼‰"""
    if not amazon_title:
        return {"score": 0, "details": ["Amazonå•†å“åãªã—"], "extracted_info": extracted_info}
    
    score = 0
    details = []
    
    original_clean = original_title.lower()
    amazon_clean = amazon_title.lower()
    
    # 1. å®Œå…¨ä¸€è‡´ãƒœãƒ¼ãƒŠã‚¹ï¼ˆæœ€å¤§40ç‚¹ï¼‰
    if original_clean == amazon_clean:
        score += 40
        details.append("å®Œå…¨ä¸€è‡´: +40ç‚¹")
    elif original_clean in amazon_clean or amazon_clean in original_clean:
        score += 25
        details.append("éƒ¨åˆ†å®Œå…¨ä¸€è‡´: +25ç‚¹")
    
    # 2. ãƒ–ãƒ©ãƒ³ãƒ‰ä¸€è‡´ï¼ˆæœ€å¤§25ç‚¹ï¼‰
    if extracted_info.get("brand") and amazon_brand:
        brand_lower = extracted_info["brand"].lower()
        amazon_brand_lower = amazon_brand.lower()
        
        if brand_lower in amazon_brand_lower or amazon_brand_lower in brand_lower:
            score += 25
            details.append(f"ãƒ–ãƒ©ãƒ³ãƒ‰ä¸€è‡´({extracted_info['brand']}): +25ç‚¹")
        elif any(brand_var.lower() in amazon_brand_lower 
                for brand_var in load_brand_dict().get(extracted_info["brand"], [])):
            score += 20
            details.append(f"ãƒ–ãƒ©ãƒ³ãƒ‰éƒ¨åˆ†ä¸€è‡´({extracted_info['brand']}): +20ç‚¹")
    
    # 3. æ•°é‡æƒ…å ±ä¸€è‡´ï¼ˆæœ€å¤§15ç‚¹ï¼‰
    if extracted_info.get("quantity"):
        if extracted_info["quantity"] in amazon_title:
            score += 15
            details.append(f"æ•°é‡ä¸€è‡´({extracted_info['quantity']}): +15ç‚¹")
        else:
            # æ•°å€¤éƒ¨åˆ†ã®ã¿ä¸€è‡´ãƒã‚§ãƒƒã‚¯
            quantity_num = re.search(r'\d+', extracted_info["quantity"])
            if quantity_num and quantity_num.group() in amazon_title:
                score += 8
                details.append(f"æ•°é‡éƒ¨åˆ†ä¸€è‡´({quantity_num.group()}): +8ç‚¹")
    
def split_japanese_words(text):
    """æ—¥æœ¬èªã®å˜èªåˆ†å‰²æ”¹å–„é–¢æ•°"""
    if not text:
        return []
    
    # ä¸€èˆ¬çš„ãªåŒ–ç²§å“ãƒ»ãƒ˜ã‚¢ã‚±ã‚¢ç”¨èªã®åˆ†å‰²ãƒ‘ã‚¿ãƒ¼ãƒ³
    compound_patterns = [
        # ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°é–¢é€£
        (r'ãƒã‚¤ãƒ«ãƒ‰ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ã‚ªã‚¤ãƒ«', ['ãƒã‚¤ãƒ«ãƒ‰', 'ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°', 'ã‚ªã‚¤ãƒ«']),
        (r'ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ã‚ªã‚¤ãƒ«', ['ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°', 'ã‚ªã‚¤ãƒ«']),
        (r'ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ã‚¯ãƒªãƒ¼ãƒ ', ['ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°', 'ã‚¯ãƒªãƒ¼ãƒ ']),
        (r'ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ãƒŸãƒ«ã‚¯', ['ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°', 'ãƒŸãƒ«ã‚¯']),
        
        # ãƒ˜ã‚¢ã‚±ã‚¢é–¢é€£
        (r'ãƒ˜ã‚¢ãƒˆãƒªãƒ¼ãƒˆãƒ¡ãƒ³ãƒˆ', ['ãƒ˜ã‚¢', 'ãƒˆãƒªãƒ¼ãƒˆãƒ¡ãƒ³ãƒˆ']),
        (r'ãƒ˜ã‚¢ã‚ªã‚¤ãƒ«', ['ãƒ˜ã‚¢', 'ã‚ªã‚¤ãƒ«']),
        (r'ãƒ˜ã‚¢ãƒŸãƒ«ã‚¯', ['ãƒ˜ã‚¢', 'ãƒŸãƒ«ã‚¯']),
        (r'ã‚·ãƒ£ãƒ³ãƒ—ãƒ¼ãƒˆãƒªãƒ¼ãƒˆãƒ¡ãƒ³ãƒˆ', ['ã‚·ãƒ£ãƒ³ãƒ—ãƒ¼', 'ãƒˆãƒªãƒ¼ãƒˆãƒ¡ãƒ³ãƒˆ']),
        
        # ä¸€èˆ¬çš„ãªè¤‡åˆèª
        (r'ã‚¹ã‚­ãƒ³ã‚±ã‚¢', ['ã‚¹ã‚­ãƒ³', 'ã‚±ã‚¢']),
        (r'ãƒ•ã‚§ã‚¤ã‚¹ã‚±ã‚¢', ['ãƒ•ã‚§ã‚¤ã‚¹', 'ã‚±ã‚¢']),
        (r'ãƒœãƒ‡ã‚£ã‚±ã‚¢', ['ãƒœãƒ‡ã‚£', 'ã‚±ã‚¢']),
        (r'ãƒ“ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ¼ã‚±ã‚¢', ['ãƒ“ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ¼', 'ã‚±ã‚¢']),
    ]
    
    # å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å˜èªã‚’æŠ½å‡º
    words = []
    remaining_text = text
    
    # è¤‡åˆèªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
    for pattern, split_words in compound_patterns:
        if re.search(pattern, remaining_text):
            words.extend(split_words)
            remaining_text = re.sub(pattern, '', remaining_text)
    
    # æ®‹ã‚Šã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰é€šå¸¸ã®å˜èªã‚’æŠ½å‡º
    remaining_words = re.findall(r'\w{2,}', remaining_text)
    words.extend(remaining_words)
    
    # é‡è¤‡é™¤å»ã¨é•·ã•ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
    unique_words = []
    for word in words:
        if word and len(word) >= 2 and word not in unique_words:
            unique_words.append(word)
    
    return unique_words

def calculate_enhanced_relevance_score(original_title, amazon_title, amazon_brand, extracted_info):
    """æ”¹è‰¯ã•ã‚ŒãŸä¸€è‡´åº¦è¨ˆç®—ï¼ˆæ—¥æœ¬èªå˜èªåˆ†å‰²å¯¾å¿œãƒ»æœ€å¤§100ç‚¹ï¼‰"""
    if not amazon_title:
        return {"score": 0, "details": ["Amazonå•†å“åãªã—"], "extracted_info": extracted_info}
    
    score = 0
    details = []
    
    original_clean = original_title.lower()
    amazon_clean = amazon_title.lower()
    
    # 1. å®Œå…¨ä¸€è‡´ãƒœãƒ¼ãƒŠã‚¹ï¼ˆæœ€å¤§40ç‚¹ï¼‰
    if original_clean == amazon_clean:
        score += 40
        details.append("å®Œå…¨ä¸€è‡´: +40ç‚¹")
    elif original_clean in amazon_clean or amazon_clean in original_clean:
        score += 25
        details.append("éƒ¨åˆ†å®Œå…¨ä¸€è‡´: +25ç‚¹")
    
    # 2. ãƒ–ãƒ©ãƒ³ãƒ‰ä¸€è‡´ï¼ˆæœ€å¤§25ç‚¹ï¼‰
    if extracted_info.get("brand") and amazon_brand:
        brand_lower = extracted_info["brand"].lower()
        amazon_brand_lower = amazon_brand.lower()
        
        if brand_lower in amazon_brand_lower or amazon_brand_lower in brand_lower:
            score += 25
            details.append(f"ãƒ–ãƒ©ãƒ³ãƒ‰ä¸€è‡´({extracted_info['brand']}): +25ç‚¹")
        elif any(brand_var.lower() in amazon_brand_lower 
                for brand_var in load_brand_dict().get(extracted_info["brand"], [])):
            score += 20
            details.append(f"ãƒ–ãƒ©ãƒ³ãƒ‰éƒ¨åˆ†ä¸€è‡´({extracted_info['brand']}): +20ç‚¹")
    
    # 3. æ•°é‡æƒ…å ±ä¸€è‡´ï¼ˆæœ€å¤§15ç‚¹ï¼‰
    if extracted_info.get("quantity"):
        if extracted_info["quantity"] in amazon_title:
            score += 15
            details.append(f"æ•°é‡ä¸€è‡´({extracted_info['quantity']}): +15ç‚¹")
        else:
            # æ•°å€¤éƒ¨åˆ†ã®ã¿ä¸€è‡´ãƒã‚§ãƒƒã‚¯
            quantity_num = re.search(r'\d+', extracted_info["quantity"])
            if quantity_num and quantity_num.group() in amazon_title:
                score += 8
                details.append(f"æ•°é‡éƒ¨åˆ†ä¸€è‡´({quantity_num.group()}): +8ç‚¹")
    
    # 4. æ”¹è‰¯ã•ã‚ŒãŸå˜èªä¸€è‡´ï¼ˆæ—¥æœ¬èªå¯¾å¿œãƒ»æœ€å¤§35ç‚¹ï¼‰
    # é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å®šç¾©ï¼ˆè‹±æ—¥å¯¾è¨³ä»˜ãï¼‰
    important_keywords = {
        # è‹±èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        'cleansing', 'oil', 'mild', 'cream', 'lotion', 'serum',
        'essence', 'toner', 'milk', 'moisturizing', 'beauty',
        'face', 'skin', 'care', 'makeup', 'foundation',
        'shampoo', 'treatment', 'conditioner', 'hair', 'scalp', 
        'repair', 'damage', 'volume', 'shine',
        
        # æ—¥æœ¬èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        'ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°', 'ã‚ªã‚¤ãƒ«', 'ãƒã‚¤ãƒ«ãƒ‰', 'ã‚¯ãƒªãƒ¼ãƒ ', 'ãƒ­ãƒ¼ã‚·ãƒ§ãƒ³', 
        'ã‚»ãƒ©ãƒ ', 'ã‚¨ãƒƒã‚»ãƒ³ã‚¹', 'ãƒˆãƒŠãƒ¼', 'ãƒŸãƒ«ã‚¯', 'ä¿æ¹¿', 'ãƒ“ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ¼',
        'ãƒ•ã‚§ã‚¤ã‚¹', 'ã‚¹ã‚­ãƒ³', 'ã‚±ã‚¢', 'ãƒ¡ã‚¤ã‚¯', 'ãƒ•ã‚¡ãƒ³ãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³',
        'ã‚·ãƒ£ãƒ³ãƒ—ãƒ¼', 'ãƒˆãƒªãƒ¼ãƒˆãƒ¡ãƒ³ãƒˆ', 'ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒŠãƒ¼', 'ãƒ˜ã‚¢', 
        'ã‚¹ã‚«ãƒ«ãƒ—', 'ãƒªãƒšã‚¢', 'ãƒ€ãƒ¡ãƒ¼ã‚¸', 'ãƒœãƒªãƒ¥ãƒ¼ãƒ '
    }
    
    # è‹±æ—¥å¯¾è¨³è¾æ›¸ï¼ˆé‡è¦ï¼‰
    en_jp_dict = {
        'cleansing': 'ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°',
        'oil': 'ã‚ªã‚¤ãƒ«',
        'mild': 'ãƒã‚¤ãƒ«ãƒ‰',
        'cream': 'ã‚¯ãƒªãƒ¼ãƒ ',
        'lotion': 'ãƒ­ãƒ¼ã‚·ãƒ§ãƒ³',
        'serum': 'ã‚»ãƒ©ãƒ ',
        'essence': 'ã‚¨ãƒƒã‚»ãƒ³ã‚¹',
        'milk': 'ãƒŸãƒ«ã‚¯',
        'shampoo': 'ã‚·ãƒ£ãƒ³ãƒ—ãƒ¼',
        'treatment': 'ãƒˆãƒªãƒ¼ãƒˆãƒ¡ãƒ³ãƒˆ',
        'conditioner': 'ã‚³ãƒ³ãƒ‡ã‚£ã‚·ãƒ§ãƒŠãƒ¼',
        'hair': 'ãƒ˜ã‚¢',
        'care': 'ã‚±ã‚¢'
    }
    
    # æ”¹è‰¯ã•ã‚ŒãŸå˜èªæŠ½å‡º
    # å…ƒã®å•†å“åï¼ˆæ—¥æœ¬èªå¯¾å¿œå˜èªåˆ†å‰²ï¼‰
    if re.search(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FFF]', original_title):
        # æ—¥æœ¬èªãŒå«ã¾ã‚Œã‚‹å ´åˆã¯æ—¥æœ¬èªå˜èªåˆ†å‰²ã‚’ä½¿ç”¨
        original_words = set(split_japanese_words(original_title))
    else:
        # è‹±èªã®å ´åˆã¯é€šå¸¸ã®åˆ†å‰²
        original_words = set(re.findall(r'\b\w{3,}\b', original_clean))
    
    # Amazonå•†å“åï¼ˆæ—¥æœ¬èªå¯¾å¿œå˜èªåˆ†å‰²ï¼‰
    amazon_words = set(split_japanese_words(amazon_title)) | set(re.findall(r'\b\w{2,}\b', amazon_clean))
    
    # å…±é€šå˜èªã‚’å–å¾—
    common_words = original_words & amazon_words
    
    # è‹±æ—¥å¯¾è¨³ã§ã®ä¸€è‡´ã‚‚ãƒã‚§ãƒƒã‚¯
    translated_matches = []
    for en_word in original_words:
        jp_word = en_jp_dict.get(en_word.lower())
        if jp_word and jp_word in amazon_title:
            translated_matches.append(f"{en_word}â†’{jp_word}")
            common_words.add(en_word)  # å¯¾è¨³ä¸€è‡´ã¨ã—ã¦è¿½åŠ 
    
    # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
    print(f"   ğŸ” æ”¹è‰¯ç‰ˆå˜èªåˆ†æ:")
    print(f"      å…ƒã®å˜èª: {sorted(original_words)}")
    print(f"      Amazonå˜èª: {sorted(amazon_words)}")
    print(f"      å…±é€šå˜èª: {sorted(common_words)}")
    print(f"      è‹±æ—¥å¯¾è¨³ä¸€è‡´: {translated_matches}")
    
    # é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨ã®ä¸€è‡´ã‚’ãƒã‚§ãƒƒã‚¯
    matched_important = []
    matched_general = []
    
    for word in common_words:
        if word.lower() in important_keywords:
            matched_important.append(word)
        else:
            matched_general.append(word)
    
    print(f"      é‡è¦å˜èª: {matched_important}")
    print(f"      ä¸€èˆ¬å˜èª: {matched_general}")
    
    # é‡è¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯é«˜å¾—ç‚¹
    if matched_important:
        important_score = min(len(matched_important) * 5, 20)  # é‡è¦èª1å€‹=5ç‚¹ã€æœ€å¤§20ç‚¹
        score += important_score
        details.append(f"é‡è¦å˜èªä¸€è‡´({len(matched_important)}å€‹: {', '.join(matched_important)}): +{important_score}ç‚¹")
    
    # è‹±æ—¥å¯¾è¨³ä¸€è‡´ã®è¿½åŠ å¾—ç‚¹
    if translated_matches:
        translation_score = min(len(translated_matches) * 8, 15)  # å¯¾è¨³1å€‹=8ç‚¹ã€æœ€å¤§15ç‚¹
        score += translation_score
        details.append(f"è‹±æ—¥å¯¾è¨³ä¸€è‡´({len(translated_matches)}å€‹: {', '.join(translated_matches)}): +{translation_score}ç‚¹")
    
    # ä¸€èˆ¬çš„ãªå˜èªã‚‚åŠ ç‚¹
    if matched_general:
        general_score = min(len(matched_general) * 2, 10)  # ä¸€èˆ¬èª1å€‹=2ç‚¹ã€æœ€å¤§10ç‚¹
        score += general_score
        details.append(f"ä¸€èˆ¬å˜èªä¸€è‡´({len(matched_general)}å€‹: {', '.join(matched_general)}): +{general_score}ç‚¹")
    
    # 5. å•†å“ã‚¿ã‚¤ãƒ—ä¸€è‡´ãƒœãƒ¼ãƒŠã‚¹ï¼ˆæœ€å¤§10ç‚¹ï¼‰
    product_types = {
        'cleansing': ['ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°', 'ãƒ¡ã‚¤ã‚¯è½ã¨ã—'],
        'oil': ['ã‚ªã‚¤ãƒ«'],
        'cream': ['ã‚¯ãƒªãƒ¼ãƒ '],
        'lotion': ['ãƒ­ãƒ¼ã‚·ãƒ§ãƒ³', 'åŒ–ç²§æ°´'],
        'serum': ['ã‚»ãƒ©ãƒ ', 'ç¾å®¹æ¶²'],
        'shampoo': ['ã‚·ãƒ£ãƒ³ãƒ—ãƒ¼'],
        'treatment': ['ãƒˆãƒªãƒ¼ãƒˆãƒ¡ãƒ³ãƒˆ'],
        'milk': ['ãƒŸãƒ«ã‚¯', 'ä¹³æ¶²']
    }
    
    type_matches = 0
    for eng_type, jp_types in product_types.items():
        if eng_type in original_clean:
            for jp_type in jp_types:
                if jp_type in amazon_title:
                    type_matches += 1
                    break
    
    if type_matches > 0:
        type_score = min(type_matches * 5, 10)
        score += type_score
        details.append(f"å•†å“ã‚¿ã‚¤ãƒ—ä¸€è‡´({type_matches}å€‹): +{type_score}ç‚¹")
    
    # æœ€å¤§100ç‚¹ã«åˆ¶é™
    score = min(score, 100)
    
    return {
        "score": score,
        "details": details,
        "extracted_info": extracted_info
    }

def search_asin_with_prime_priority(title, max_results=5, **kwargs):
    """Primeå„ªå…ˆASINæ¤œç´¢ï¼ˆGPT-4oæ—¥æœ¬èªåŒ–çµ±åˆç‰ˆï¼‰"""
    # extracted_info ã‚’æœ€åˆã«åˆæœŸåŒ–ï¼ˆã‚¨ãƒ©ãƒ¼å‡¦ç†ã§å‚ç…§ã™ã‚‹ãŸã‚ï¼‰
    extracted_info = {"brand": None, "quantity": None, "cleaned_text": str(title)}
    
    credentials = get_credentials()
    if not credentials:
        return {"search_status": "auth_error", "error": "èªè¨¼æƒ…å ±ãŒå–å¾—ã§ãã¾ã›ã‚“", "extracted_info": extracted_info}
    
    try:
        print(f"ğŸ” GPT-4oæ—¥æœ¬èªåŒ–å¯¾å¿œæ¤œç´¢é–‹å§‹")
        print(f"   å…ƒã®å•†å“å: {title}")
        
        # ã‚¹ãƒ†ãƒƒãƒ—1: å•†å“åã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
        brand_dict = load_brand_dict()
        extracted_info = extract_brand_and_quantity(title, brand_dict)
        cleaned_title = extracted_info["cleaned_text"]
        
        print(f"   ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å¾Œ: {cleaned_title}")
        print(f"   æŠ½å‡ºãƒ–ãƒ©ãƒ³ãƒ‰: {extracted_info.get('brand', 'ãªã—')}")
        print(f"   æŠ½å‡ºæ•°é‡: {extracted_info.get('quantity', 'ãªã—')}")
        
        # ã‚¹ãƒ†ãƒƒãƒ—2: GPT-4oæ—¥æœ¬èªåŒ–ï¼ˆè‹±èªã®å ´åˆã®ã¿ï¼‰
        search_query = cleaned_title
        japanese_name = None
        llm_source = "Not Applied"
        
        # è‹±èªãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯æ—¥æœ¬èªåŒ–ã‚’å®Ÿè¡Œ
        english_detected = re.search(r'[a-zA-Z]', cleaned_title)
        print(f"   ğŸ” è‹±èªæ¤œå‡ºãƒã‚§ãƒƒã‚¯: '{cleaned_title}' â†’ è‹±èªã‚ã‚Š: {bool(english_detected)}")
        
        if english_detected:
            print(f"   ğŸ¤– è‹±èªæ¤œå‡ºã€GPT-4oæ—¥æœ¬èªåŒ–å®Ÿè¡Œä¸­...")
            japanese_name, llm_source = get_japanese_name_hybrid(cleaned_title)
            
            if japanese_name and japanese_name != cleaned_title:
                search_query = japanese_name
                print(f"   âœ… æ—¥æœ¬èªåŒ–æˆåŠŸ: {search_query}")
            else:
                print(f"   âš ï¸ æ—¥æœ¬èªåŒ–å¤±æ•—ã€å…ƒã®ã‚¿ã‚¤ãƒˆãƒ«ã§æ¤œç´¢: {search_query}")
        else:
            print(f"   â„¹ï¸ æ—¥æœ¬èªå•†å“åã€ãã®ã¾ã¾æ¤œç´¢: {search_query}")
        
        print(f"   ğŸ” æœ€çµ‚æ¤œç´¢ã‚¯ã‚¨ãƒª: '{search_query}'")
        
        # ã‚¹ãƒ†ãƒƒãƒ—3: SP-APIæ¤œç´¢å®Ÿè¡Œ
        catalog_api = CatalogItems(
            credentials=credentials,
            marketplace=Marketplaces.JP
        )
        
        response = catalog_api.search_catalog_items(
            keywords=search_query,
            pageSize=max_results,
            marketplaceIds=[Marketplaces.JP.marketplace_id]
        )
        
        items = response.payload.get('items', [])
        if not items:
            return {
                "search_status": "no_results",
                "error": f"æ¤œç´¢çµæœãªã—: {search_query}",
                "extracted_info": extracted_info,
                "japanese_name": japanese_name,
                "llm_source": llm_source
            }
        
        # ãƒ‡ãƒãƒƒã‚°ç”¨: ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹é€ ã‚’å‡ºåŠ›
        print(f"ğŸ“Š SP-API ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹é€ ç¢ºèª:")
        print(f"   ç·ä»¶æ•°: {len(items)}ä»¶")
        print(f"   æ¤œç´¢ã‚¯ã‚¨ãƒª: {search_query}")
        
        # æœ€é©ãªã‚¢ã‚¤ãƒ†ãƒ ã‚’é¸æŠï¼ˆæ”¹è‰¯ã•ã‚ŒãŸã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ï¼‰
        best_item = None
        best_score = 0
        
        for item in items:
            asin = item.get('asin', '')
            
            # SP-APIãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹é€ ã«åˆã‚ã›ã¦æŸ”è»Ÿã«å¯¾å¿œ
            item_title = ""
            brand_info = ""
            
            # è¤‡æ•°ã®ã‚­ãƒ¼ã‚’è©¦è¡Œã—ã¦å•†å“åã‚’å–å¾—
            for title_key in ['itemName', 'title', 'name', 'productTitle']:
                if item.get(title_key):
                    item_title = item.get(title_key)
                    break
            
            # è¤‡æ•°ã®ã‚­ãƒ¼ã‚’è©¦è¡Œã—ã¦ãƒ–ãƒ©ãƒ³ãƒ‰åã‚’å–å¾—
            for brand_key in ['brand', 'brandName', 'manufacturer']:
                if item.get(brand_key):
                    brand_info = item.get(brand_key)
                    break
            
            # summariesã‚­ãƒ¼ãŒã‚ã‚‹å ´åˆã¯å¾“æ¥ã®æ–¹æ³•ã‚‚è©¦è¡Œ
            if not item_title and 'summaries' in item:
                summaries = item.get('summaries', [])
                if summaries and isinstance(summaries, list):
                    summary = summaries[0]
                    item_title = summary.get('itemName', '') or summary.get('title', '')
                    brand_info = summary.get('brand', '') or summary.get('brandName', '')
            
            print(f"   ğŸ” ã‚¢ã‚¤ãƒ†ãƒ è©³ç´°:")
            print(f"      ASIN: {asin}")
            print(f"      å•†å“å: {item_title[:50]}...")
            print(f"      ãƒ–ãƒ©ãƒ³ãƒ‰: {brand_info}")
            
            # æ”¹è‰¯ã•ã‚ŒãŸä¸€è‡´åº¦è¨ˆç®—ï¼ˆæ—¥æœ¬èªåŒ–å¾Œã®ã‚¿ã‚¤ãƒˆãƒ«ã¨æ¯”è¼ƒï¼‰
            relevance_result = calculate_enhanced_relevance_score(
                search_query, item_title, brand_info, extracted_info
            )
            
            if relevance_result["score"] > best_score:
                best_score = relevance_result["score"]
                best_item = {
                    "asin": asin,
                    "title": item_title,
                    "brand": brand_info,
                    "relevance_score": relevance_result["score"],
                    "relevance_details": relevance_result["details"],
                    "is_prime": False,  # ç°¡æ˜“ç‰ˆã§ã¯Primeåˆ¤å®šçœç•¥
                    "price": "unknown",
                    "extracted_info": extracted_info,
                    "japanese_name": japanese_name,
                    "llm_source": llm_source
                }
        
        if best_item:
            print(f"   âœ… æˆåŠŸ: ASIN={best_item['asin']}")
            print(f"      å•†å“å: {best_item['title']}")
            print(f"      ãƒ–ãƒ©ãƒ³ãƒ‰: {best_item['brand']}")
            print(f"      ä¸€è‡´åº¦: {best_item['relevance_score']}%")
            print(f"      æ—¥æœ¬èªåŒ–: {japanese_name} ({llm_source})")
            print(f"      è©³ç´°: {', '.join(best_item['relevance_details'])}")
            
            return {
                "search_status": "success",
                "asin": best_item["asin"],
                "title": best_item["title"],
                "brand": best_item["brand"],
                "relevance_score": best_item["relevance_score"],
                "relevance_details": best_item["relevance_details"],
                "is_prime": best_item["is_prime"],
                "price": best_item["price"],
                "extracted_info": extracted_info,
                "japanese_name": japanese_name,
                "llm_source": llm_source
            }
        else:
            return {
                "search_status": "low_relevance",
                "error": "é–¢é€£æ€§ã®é«˜ã„å•†å“ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ",
                "extracted_info": extracted_info,
                "japanese_name": japanese_name,
                "llm_source": llm_source
            }
            
    except SellingApiException as e:
        error_msg = f"SP-API ã‚¨ãƒ©ãƒ¼: {e.code} - {e.message}"
        print(f"   âŒ å¤±æ•—: {error_msg}")
        return {"search_status": "api_error", "error": error_msg, "extracted_info": extracted_info}
    except Exception as e:
        error_msg = f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)}"
        print(f"   âŒ å¤±æ•—: {error_msg}")
        return {"search_status": "error", "error": error_msg, "extracted_info": extracted_info}

def process_batch_asin_search_with_ui(df, title_column='clean_title', limit=None):
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ UIä»˜ããƒãƒƒãƒASINæ¤œç´¢ï¼ˆæ—¢å­˜æ©Ÿèƒ½å®Œå…¨çµ±åˆç‰ˆï¼‰"""
    # å‡¦ç†å¯¾è±¡ã®æ±ºå®š
    if limit:
        df_to_process = df.head(limit).copy()
    else:
        df_to_process = df.copy()
    
    total_items = len(df_to_process)
    
    print(f"ğŸš€ æ—¢å­˜æ©Ÿèƒ½çµ±åˆç‰ˆãƒãƒƒãƒASINæ¤œç´¢é–‹å§‹: {total_items}ä»¶ã®å•†å“ã‚’å‡¦ç†")
    print(f"ğŸ“Š çµ±åˆæ©Ÿèƒ½:")
    print(f"   âœ… é«˜å“è³ªå•†å“åã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°")
    print(f"   âœ… 500+ãƒ–ãƒ©ãƒ³ãƒ‰è¾æ›¸æ´»ç”¨")
    print(f"   âœ… æ”¹è‰¯ã•ã‚ŒãŸä¸€è‡´åº¦è¨ˆç®—")
    print(f"   âœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—è¡¨ç¤º")
    
    # UIè¦ç´ ã®åˆæœŸåŒ–
    progress_bar = st.progress(0)
    status_container = st.container()
    metrics_container = st.container()
    current_item_container = st.container()
    log_container = st.container()
    
    # çµæœã‚«ãƒ©ãƒ ã®åˆæœŸåŒ–
    result_columns = [
        'amazon_asin', 'amazon_title', 'amazon_brand', 'relevance_score',
        'is_prime', 'price', 'search_status', 'extracted_brand', 
        'extracted_quantity', 'cleaned_title', 'relevance_details',
        'japanese_name', 'llm_source'  # æ—¥æœ¬èªåŒ–æƒ…å ±ã‚’è¿½åŠ 
    ]
    
    for col in result_columns:
        if col not in df_to_process.columns:
            df_to_process[col] = ""
    
    # ãƒãƒƒãƒå‡¦ç†å®Ÿè¡Œ
    success_count = 0
    error_count = 0
    detailed_logs = []
    
    for idx, row in df_to_process.iterrows():
        current_progress = (idx + 1) / total_items
        progress_bar.progress(current_progress)
        
        # ç¾åœ¨ã®å‡¦ç†çŠ¶æ³è¡¨ç¤º
        with current_item_container:
            st.write(f"ğŸ” {idx + 1}/{total_items}: æ¤œç´¢ä¸­")
            current_title = str(row[title_column])[:100] + ("..." if len(str(row[title_column])) > 100 else "")
            st.write(f"å•†å“å: {current_title}")
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
        with metrics_container:
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("å‡¦ç†æ¸ˆã¿", f"{idx + 1}/{total_items}")
            with col2:
                st.metric("æˆåŠŸ", f"{success_count}")
            with col3:
                st.metric("å¤±æ•—", f"{error_count}")
            with col4:
                success_rate = (success_count / (idx + 1)) * 100 if idx >= 0 else 0
                st.metric("æˆåŠŸç‡", f"{success_rate:.1f}%")
        
        # ASINæ¤œç´¢å®Ÿè¡Œ
        search_result = search_asin_with_prime_priority(str(row[title_column]))
        
        # çµæœã®å‡¦ç†
        if search_result.get("search_status") == "success":
            success_count += 1
            df_to_process.at[idx, 'amazon_asin'] = search_result['asin']
            df_to_process.at[idx, 'amazon_title'] = search_result['title']
            df_to_process.at[idx, 'amazon_brand'] = search_result.get('brand', '')
            df_to_process.at[idx, 'relevance_score'] = search_result['relevance_score']
            df_to_process.at[idx, 'is_prime'] = search_result.get('is_prime', False)
            df_to_process.at[idx, 'price'] = search_result.get('price', 'unknown')
            df_to_process.at[idx, 'search_status'] = 'success'
            
            # æŠ½å‡ºæƒ…å ±
            extracted_info = search_result.get('extracted_info', {})
            df_to_process.at[idx, 'extracted_brand'] = extracted_info.get('brand', '')
            df_to_process.at[idx, 'extracted_quantity'] = extracted_info.get('quantity', '')
            df_to_process.at[idx, 'cleaned_title'] = extracted_info.get('cleaned_text', '')
            df_to_process.at[idx, 'relevance_details'] = ', '.join(search_result.get('relevance_details', []))
            
            # æ—¥æœ¬èªåŒ–æƒ…å ±
            df_to_process.at[idx, 'japanese_name'] = search_result.get('japanese_name', '')
            df_to_process.at[idx, 'llm_source'] = search_result.get('llm_source', '')
            
            log_entry = f"âœ… {idx + 1}/{total_items}: {search_result['asin']} - {search_result['title'][:50]}... (æ—¥æœ¬èª: {search_result.get('japanese_name', 'ãªã—')})"
        else:
            error_count += 1
            df_to_process.at[idx, 'search_status'] = search_result.get('search_status', 'error')
            # æ—¥æœ¬èªåŒ–æƒ…å ±ãŒã‚ã‚Œã°è¨˜éŒ²
            df_to_process.at[idx, 'japanese_name'] = search_result.get('japanese_name', '')
            df_to_process.at[idx, 'llm_source'] = search_result.get('llm_source', '')
            error_reason = search_result.get('error', 'ä¸æ˜ãªã‚¨ãƒ©ãƒ¼')
            log_entry = f"âŒ {idx + 1}/{total_items}: {error_reason}"
        
        detailed_logs.append(log_entry)
        
        # ãƒ­ã‚°è¡¨ç¤ºæ›´æ–°
        with log_container:
            with st.expander("ğŸ“‹ è©³ç´°ãƒ­ã‚°", expanded=False):
                for log in detailed_logs[-10:]:  # æœ€æ–°10ä»¶ã®ã¿è¡¨ç¤º
                    st.text(log)
        
        # APIåˆ¶é™å¯¾ç­–
        time.sleep(1)
    
    # æœ€çµ‚çµæœè¡¨ç¤º
    final_success_rate = (success_count / total_items) * 100
    
    with status_container:
        st.success(f"ğŸ‰ ãƒãƒƒãƒå‡¦ç†å®Œäº†: {success_count}/{total_items}ä»¶æˆåŠŸ (æˆåŠŸç‡: {final_success_rate:.1f}%)")
    
    print(f"ğŸ¯ æ—¢å­˜æ©Ÿèƒ½çµ±åˆç‰ˆãƒãƒƒãƒå‡¦ç†å®Œäº†:")
    print(f"   ğŸ“Š å…¨ä»¶æ•°: {total_items}ä»¶")
    print(f"   âœ… æˆåŠŸ: {success_count}ä»¶")
    print(f"   âŒ å¤±æ•—: {error_count}ä»¶")
    print(f"   ğŸ“ˆ æˆåŠŸç‡: {final_success_rate:.1f}%")
    
    # æˆåŠŸæ™‚ã®åŠ¹æœ
    if success_count > 0:
        st.balloons()
    
    return df_to_process

def process_batch_asin_search(df, title_column='clean_title', limit=None):
    """ãƒãƒƒãƒASINæ¤œç´¢ï¼ˆä¸‹ä½äº’æ›æ€§ç¶­æŒï¼‰"""
    return process_batch_asin_search_with_ui(df, title_column, limit)

def test_sp_api_connection():
    """SP-APIæ¥ç¶šãƒ†ã‚¹ãƒˆ"""
    print("ğŸ§ª SP-APIæ¥ç¶šãƒ†ã‚¹ãƒˆ")
    credentials = get_credentials()
    
    if not credentials:
        print("âŒ èªè¨¼æƒ…å ±ã®å–å¾—ã«å¤±æ•—")
        return False
    
    try:
        catalog_api = CatalogItems(
            credentials=credentials,
            marketplace=Marketplaces.JP
        )
        
        # ãƒ†ã‚¹ãƒˆæ¤œç´¢å®Ÿè¡Œï¼ˆSP-APIãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¿®æ­£ï¼‰
        response = catalog_api.search_catalog_items(
            keywords="ãƒ†ã‚¹ãƒˆ",
            pageSize=1,
            marketplaceIds=[Marketplaces.JP.marketplace_id]
        )
        
        print("âœ… SP-APIæ¥ç¶šãƒ†ã‚¹ãƒˆæˆåŠŸ")
        return True
        
    except Exception as e:
        print(f"âŒ SP-APIæ¥ç¶šãƒ†ã‚¹ãƒˆå¤±æ•—: {e}")
        return False

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
if __name__ == "__main__":
    print("=== SP-API Service æ—¢å­˜æ©Ÿèƒ½çµ±åˆç‰ˆãƒ†ã‚¹ãƒˆ ===")
    
    # èªè¨¼æƒ…å ±ãƒ†ã‚¹ãƒˆ
    result = test_sp_api_connection()
    print(f"æ¥ç¶šãƒ†ã‚¹ãƒˆçµæœ: {result}")
    
    # é«˜å“è³ªã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ãƒ†ã‚¹ãƒˆ
    test_title = "ğŸ…¹ğŸ…¿ğŸ‡¯ğŸ‡µ Japan Fancl Mild Cleansing Oil 120ml*2 100% Authentic made in japan original"
    cleaned = advanced_product_name_cleansing(test_title)
    print(f"ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ãƒ†ã‚¹ãƒˆ:")
    print(f"  å…ƒ: {test_title}")
    print(f"  å¾Œ: {cleaned}")
    
    # ãƒ–ãƒ©ãƒ³ãƒ‰ãƒ»æ•°é‡æŠ½å‡ºãƒ†ã‚¹ãƒˆ
    brand_dict = load_brand_dict()
    extracted = extract_brand_and_quantity(test_title, brand_dict)
    print(f"æŠ½å‡ºãƒ†ã‚¹ãƒˆ:")
    print(f"  ãƒ–ãƒ©ãƒ³ãƒ‰: {extracted['brand']}")
    print(f"  æ•°é‡: {extracted['quantity']}")
    print(f"  ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å¾Œ: {extracted['cleaned_text']}")